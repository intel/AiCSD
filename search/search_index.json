{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"AI Connect for Scientific Data (AiCSD)","text":"<p>The AiCSD reference implementation manages pipelines for image processing and automated image comparisons. Adaptable for different use cases and settings, the reference implementation offers an architecture of microservices connected by a secure Redis Message Broker and various communication APIs. This flexible architecture can be customized for many applications, such as biopharma, agricultural, and industrial.</p> <p>Figure 1 below shows the services created for this reference implementation and the flow of data through the architecture. </p> Figure 1: Architecture and High-level Dataflow <p>The architectural components include: </p> <ul> <li>Microservices: Provided by Intel, the microservices include a user interface and applications for managing files and jobs.</li> <li>EdgeX Application Services: The reference implementation uses the APIs from the EdgeX Applications Services to communicate and transfer information. </li> <li>EdgeX Services: The services include the database, message broker, and security services.</li> <li>Pipeline Execution: The reference implementation furnishes an example pipeline for pipeline management.</li> <li>File System: The reference implementation stores and manages input and output files.</li> <li>Third-party Input Devices: The devices supply the images that will be processed. Examples include an optical microscope or conveyor belt camera.</li> </ul> <p>Note</p> <p>Currently, the reference implementation supplies a image classification pipeline example.</p>"},{"location":"index.html#setup","title":"Setup","text":"<p>Set up a 2-system configuration, an edge system running Ubuntu and OEM equipment running WSL2 or Ubuntu.</p> <ul> <li>System Setup Completion Time: Approximately 30-90 Minutes</li> <li>Workflow Completion Time: Approximately 30-60 Minutes</li> </ul> <p>Get Started</p> <p>Note</p> <p>If coming from the catalog please follow Getting started Catalog</p>"},{"location":"index.html#additional-resources","title":"Additional Resources","text":"<p>For additional details, see the General Information.</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"LICENSE.html","title":"LICENSE","text":"<p>Copyright 2023 Intel Corporation</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \u201cAS IS\u201d AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"troubleshooting-tools.html","title":"Troubleshooting","text":""},{"location":"troubleshooting-tools.html#general","title":"General","text":"Error or Issue Possible Solution Update the .env file If there are issues seeing service and container logs within the Kibana UI or the system health data on the Grafana dashboard, verify the contents of .env file. To verify the password and token necessary for authentication, refer to the setup steps in Run the Services. Check logs To check the logs for Telegraf, InfluxDB, and/or Grafana, see Check Logs"},{"location":"troubleshooting-tools.html#authentication-issues","title":"Authentication Issues","text":"Error or Issue Possible Solution Grafana dashboard not appearing If there are issues seeing system health data on the default Grafana dashboard, check the .env file. To verify the password and token necessary for authentication, refer to the setup steps in Run the Services. InfluxDB Error Log - <code>Error: failed to setup instance: password is too short</code> InfluxDB user passwords must contain at least 8 characters. If there is a password issue, the error on the left will appear in the error log for InfluxDB."},{"location":"troubleshooting-tools.html#grafana-ui-connection","title":"Grafana UI Connection","text":"Error or Issue Possible Solution No data message in Grafana UI The error illustrated by the figure below indicates the UI has a token authorization problem. To verify, see Check Logs. The InfluxDB Error Log will contain <code>error=\"token required\", error=\"authorization not found\"</code>. To fix, see Reset Authorization Header."},{"location":"troubleshooting-tools.html#kibana-ui-connection","title":"Kibana UI Connection","text":"Error or Issue Possible Solution Kibana Error Message: <code>Kibana server is not ready yet</code>. There may be an issue with the setup time for Kibana. Give the services a little longer to finish the setup process and try refreshing the page."},{"location":"troubleshooting-tools.html#docker-startup-sequence","title":"Docker Startup Sequence","text":"Error or Issue Possible Solution <code>|[outputs.influxdb_v2] When writing to [http://influxdb:8086]: Post \"http://influxdb:8086/api/v2/write?bucket=systemHealthMonitoring&amp;org=AiCSD\": dial tcp 172.26.0.2:8086: connect: connection refused</code> <code>Telegraf</code> may start before <code>InfluxDB</code> is ready for connections. This can lead to the error message on the left in the error log.  If the error appears only once and does not repeat several times, disregard the error log."},{"location":"troubleshooting-tools.html#reset-authorization-header","title":"Reset Authorization Header","text":"<ol> <li>Click on the Sign in button on the bottom of the Grafana UI nav bar on the left. Sign in with user: admin and password: admin.  </li> <li>Change the default admin password to something other than admin. The Grafana UI will prompt for a new password. </li> <li>Click on the gear wheel in the left nav bar and go to Data Sources. </li> <li>Click on InfluxDB. </li> <li>Under the Custom HTTP Headers section, click Reset.</li> <li>In the Value section type Token , where the  is the token update in the .env file. Be sure to put the space between the Token prefix and your actual token. <li>Click Save &amp; test and refresh the browser.</li> <p>This series of steps resets the Authorization header to enable Grafana to access data for the dashboards.</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"catalog/Get-Started-Guide.html","title":"Get Started Guide","text":"<ul> <li>Time to Complete: 30 minutes</li> <li>Programming Language: Go, Python, Angular*</li> </ul>"},{"location":"catalog/Get-Started-Guide.html#get-started","title":"Get Started","text":"<p>A typical setup for the AI Connect for Scientific Data Reference Implementation (or AiCSD) comprises of two systems, a Gateway system and an OEM system, communicating with each other via a secure SSH tunnel. Scientific instruments (for example, optical microscopes) are connected to the OEM system and supply the images for processing on the Gateway system.</p> <p>This guide demonstrates the reference implementation setup on a single system, which runs the services for both OEM and Gateway. Complete this guide to confirm that your setup is working correctly and try out a basic workflow in the reference implementation.</p>"},{"location":"catalog/Get-Started-Guide.html#prerequisites-for-target-system","title":"Prerequisites for Target System","text":"<ul> <li>Intel\u00ae Core\u2122 processor</li> <li>5 GB of memory minimum</li> <li>10 GB of storage space minimum</li> <li>Internet access</li> <li>Ubuntu* 20.04 LTS Desktop or later</li> </ul> <p>Note If Ubuntu Desktop is not installed on the target system, follow the instructions from Ubuntu to install Ubuntu desktop.</p>"},{"location":"catalog/Get-Started-Guide.html#step-1-install-the-package","title":"Step 1: Install the package","text":"<p>Follow this procedure on the target system to install the package.</p> <ol> <li> <p>Download the reference implementation package: AI Connect for Scientific Data Reference Implementation</p> </li> <li> <p>In Terminal, go to the download directory and unzip the package.</p> <pre><code> unzip ai-connect-for-scientific-data-package.zip\n</code></pre> </li> <li> <p>Go to the extracted directory.</p> <pre><code> cd ai-connect-for-scientific-data-package/\n</code></pre> </li> <li> <p>Change the permission of the edgesoftware executable.</p> <pre><code> chmod 755 edgesoftware\n</code></pre> </li> <li> <p>Install the reference implementation.</p> <pre><code> ./edgesoftware install\n</code></pre> </li> <li> <p>When prompted, enter the product key. The product key is in the email you received from Intel after you downloaded the package.</p> </li> </ol> <p>When the installation is finished, you see the message \"Installation of package complete\" and the installation status for each module.</p> <p></p> <p>Figure 1: Successful installation</p> <p>Note If the Docker* installation fails because of proxy-related issues, follow the manual docker installation steps in the Troubleshooting section.</p>"},{"location":"catalog/Get-Started-Guide.html#step-2-build-and-verify","title":"Step 2: Build and verify","text":"<p>Follow this procedure to build the containers and run the reference implementation.</p> <p>Note If you are behind a proxy or running a two-system setup, please modify the GATEWAY_IP_ADDR variable in <code>AiCSD/Makefile</code> with your gateway IP address.</p> <ol> <li>Go to the reference implementation directory and build Docker containers to run in a single-system setup.<pre><code> cd AiCSD\n make docker\n</code></pre> </li> </ol> <p>Note </p> <ol> <li> <p>Run the services.</p> <pre><code>make run-sim\n</code></pre> </li> <li> <p>Verify whether the correct number of containers are running. There are 20 containers for this single-system setup.</p> <pre><code>docker ps\n</code></pre> </li> </ol> <p></p> <p>Figure 2: List of containers</p>"},{"location":"catalog/Get-Started-Guide.html#run-a-workflow-for-remote-monitoring-and-image-analysis","title":"Run a Workflow for Remote Monitoring and Image Analysis","text":"<p>The following is the procedure to run a workflow to process an image from the OEM system on a Gateway system. In a typical deployment, the OEM and Gateway are separate systems. For this demonstration, both OEM and Gateway services are on the same system.</p>"},{"location":"catalog/Get-Started-Guide.html#step-1-create-a-task-with-the-pipeline-on-the-gateway","title":"Step 1: Create a task with the pipeline on the Gateway","text":"<p>Follow this procedure to create a task that matches the input image to the pipeline.</p> <ol> <li>Launch the Web UI by entering this URL in the browser: http://target_system_IP:4200</li> </ol> <p>Note You can identify the IP address of your target system using the following command in the Terminal.</p> <pre><code>   hostname -I\n</code></pre> <ol> <li>In the Web UI, select Add Task to create a task in the Gateway.</li> </ol> <p></p> <p>Figure 3: Add task</p> <ol> <li>Complete the following fields in the Add Task page:</li> </ol> <p></p> <p>Figure 4: Task details</p> <pre><code>-   **Description:** Provide a description of the task.\n-   **Pipeline:** Select a Model Execution Pipeline.\n</code></pre> <p>For this example, select FileAndResults: Pipeline that generates output file and results.     -   Job Selector: Select matches to filter jobs based on filenames that match the value of the Filename field, or select contains to filter jobs based on filenames that contain the value of the filename field. For this example, select contains.     -   Filename: Provide the file name of the input image. For this example, use Example_Image.      -   Model Parameters: Provide the parameter details for the model. The parameters should adhere to the following json format (with quotes):</p> <pre><code>        {\"parameter\":\"value\"}\n\n    For this example, enter the following parameters:\n\n        {\"Brightness\":\"0\"}\n</code></pre> <ol> <li>Select Save to complete this procedure.</li> </ol>"},{"location":"catalog/Get-Started-Guide.html#step-2-supply-input-images-from-the-oem-microscope","title":"Step 2: Supply input images from the OEM microscope","text":"<p>Follow this procedure to supply input images.</p> <ol> <li> <p>In Terminal, go to the input directory. The system will monitor this directory for files to be processed.</p> <pre><code>cd $HOME/data/oem-files/input\n</code></pre> </li> <li> <p>Copy an image file into this directory. The Gateway will monitor and process any new image files added to this directory. </p> </li> </ol> <p>Note Earlier in Step 1, you added a task to monitor the OEM for image files that contain the name Example_Image. This means the Gateway will monitor and process images with filenames such as Example_Image_1.png and Example_Image_2.png. Other image file types supported are JPG, GIF, and TIFF.</p> <ol> <li>Check the output directory for the output file.<pre><code>cd $HOME/data/oem-files/output\n</code></pre> </li> </ol>"},{"location":"catalog/Get-Started-Guide.html#step-3-monitor-the-status-of-the-job","title":"Step 3: Monitor the status of the job","text":"<p>Follow this procedure to check the status of the task on the Gateway.</p> <ol> <li>In the Web UI, select the View Jobs tab to see a list of all jobs.</li> </ol> <p></p> <p>Figure 5: List of jobs in the View Jobs tab</p> <ol> <li>You should see the task you created earlier and its status.</li> </ol>"},{"location":"catalog/Get-Started-Guide.html#step-4-end-the-demonstration","title":"Step 4: End the demonstration","text":"<p>Follow this procedure to stop the reference implementation and end this demonstration.</p> <ol> <li> <p>Stop the reference implementation with the following command.</p> <pre><code> make down\n</code></pre> </li> <li> <p>Confirm the containers are no longer running.</p> <pre><code> docker ps\n</code></pre> </li> </ol>"},{"location":"catalog/Get-Started-Guide.html#summary","title":"Summary","text":"<p>In this guide, you installed and validated the reference implementation, with Gateway and OEM services running on a single-system setup. You also completed a basic workflow that demonstrates image processing on the Gateway system using images supplied from an OEM system.</p>"},{"location":"catalog/Get-Started-Guide.html#learn-more","title":"Learn More","text":"<ul> <li>Learn how to integrate a custom pipeline with models from Intel\u00ae Geti\u2122 platform using the Pipeline Configuration chapter in the User Guide.</li> </ul>"},{"location":"catalog/Get-Started-Guide.html#troubleshooting","title":"Troubleshooting","text":"<p>The following are options to help you resolve issues with the reference implementation.</p>"},{"location":"catalog/Get-Started-Guide.html#manual-docker-installation","title":"Manual Docker Installation","text":"<p>If you're behind a proxy, there may be connectivity issues. Use the following steps to install Docker manually:</p> <ol> <li>Follow the instructions from Docker to install Docker Engine from a package. </li> </ol> <p>Note     When installing make sure the following packages are installed containerd, docker-ce, docker-ce-cli, docker-buildx, docker-compose</p> <ol> <li> <p>Complete these post-installation steps from Docker to manage Docker as a non-root user.</p> </li> <li> <p>Use these methods from Docker to configure the Docker CLI to use proxies.</p> </li> </ol>"},{"location":"catalog/Get-Started-Guide.html#troubleshooting_1","title":"Troubleshooting","text":"<p>Refer to Troubleshooting for additional assistance.</p>"},{"location":"catalog/Get-Started-Guide.html#known-issues","title":"Known Issues","text":"<p>For known issues, see Issues on GitHub*.</p>"},{"location":"getting-started/basic-workflow.html","title":"Basic Workflow","text":"<p>The following activity will require access to both systems (if using two). These steps assume that the systems have been set up following the System Setup Guide. </p> <p>Note</p> <p>If running on a single system, all activities labeled OEM or Gateway will be completed on the same system.</p> <p>Objective: Process an image file from an OEM microscope on the Gateway system.</p> <ol> <li> <p>On the Gateway system, create a task. A task is the method by which input images are matched to a processing pipeline.</p> <p>Note</p> <p>This assumes that the pipeline(s) are running and configured. Either use the pipeline simulator or follow the Image Classification Demo documentation to set up a ML pipeline.</p> <ol> <li> <p>This can be done using one of the following methods:</p> <ul> <li>Web User Interface (UI) - follow the instructions in the Create Task section.</li> <li>Postman - use the integration-test collection here</li> <li>CURL command line tool - developer exercise based on schema (e.g. Postman collection) or API definitions for the services</li> </ul> </li> </ol> </li> <li> <p>On the OEM system, drop a file to be processed.</p> <ol> <li>Once all the services are up and running, open a File Explorer window and enter <code>$HOME/data/oem-files/input</code> into the navigation bar. This is the default folder that the system will watch for files to be processed. If no GUI is available, use <code>cd $HOME/data/oem-files/input</code> in a Terminal to navigate to the input directory.</li> <li> <p>Drop a file into the <code>$HOME/data/oem-files/input</code> folder using one of the methods below:</p> <ul> <li>Process an individual file - Drag and drop an image file using the File Explorer. If no GUI is available, copy the file using <code>cp</code> in a Terminal.</li> <li> <p>Process a directory of files - Use the copy-files make target to copy all files in a directory to the input folder.</p> <pre><code>make copy-files COPY_DIR=/path/to/dir\n</code></pre> </li> </ul> <p>Note</p> <p>The default wait time between files being copied to the folder is 30 seconds. To change this, update the SLEEP_TIME variable.</p> <pre><code>make copy-files COPY_DIR=/path/to/dir SLEEP_TIME=15\n</code></pre> <p>Note</p> <p>Be sure that the Job Selector rule from creating a task is consistent with the file name. This is what will trigger an event to process the file.</p> </li> <li> <p>Check to see if there are any files in the <code>$HOME/data/oem-files/output</code> folder.</p> <p>Note</p> <p>Depending on the chosen pipeline an output file may not be generated. For example, using the <code>results-only</code> pipeline on the pipeline simulator will not generate an output file.</p> </li> </ol> </li> <li> <p>On the Gateway system, check the status of the file and its associated job using one of the following methods:</p> <p>Note</p> <p>The status will be Complete if the job has been successfully processed. Otherwise, the Owner field will show what component is processing the job.</p> <ul> <li>Web UI - View Jobs on Web UI.</li> <li>Command Line - follow the instructions below.<ol> <li>Query for all jobs using <code>curl localhost:59784/api/v1/job | json_pp</code>. </li> <li>Search the output for the desired input file name and grab the job id.</li> <li>Copy the job id and use it to query for details on the specified job <code>curl localhost:59784/api/v1/job/&lt;job_id&gt; | json_pp</code>.</li> </ol> </li> </ul> <p>Note</p> <p>To check the status of the pipeline processing the image, check the logs of the <code>edgex-pipeline-sim-1</code> container.</p> </li> </ol> <p>If the job did not complete, see Troubleshooting.</p>"},{"location":"getting-started/basic-workflow.html#next-up","title":"Next up","text":"<p>Learn more about the UI</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"getting-started/choose-config.html","title":"Configure","text":""},{"location":"getting-started/choose-config.html#generate-keys-optional","title":"Generate Keys (Optional)","text":"<p>This is only required in a two-system setup. In order to use the SSH tunnel between the systems, it is necessary to generate SSH keys.  Only the public key will be shared to the OEM system. Each keypair should be unique to its deployment for security purposes.</p> <ol> <li>Generate ssh keys on the Gateway System: <code>make generate-ssh-tunnel-keys</code></li> <li> <p>Copy generated public key to the OEM system filling in the appropriate username for <code>&lt;oem-user&gt;</code>, system name for <code>&lt;OEM-System&gt;</code>, and path to the repository <code>/path/to</code>.     <pre><code>$ scp -r edgex-res/remote/sshd-remote/authorized_keys &lt;oem-user&gt;@&lt;OEM-System&gt;:/path/to/applications.retail.kaskey-park.kaskey-park/edgex-res/remote/sshd-remote/authorized_keys\n</code></pre></p> <p>Note</p> <p>If it is not possible to use <code>scp</code> to move the file, use a USB flash drive to move the file <code>edgex-res/remote/sshd-remote/authorized_keys</code> on the gateway to <code>/path/to/applications.retail.kaskey-park.kaskey-park/edgex-res/remote/sshd-remote/authorized_keys</code> on the OEM.</p> </li> </ol>"},{"location":"getting-started/choose-config.html#modify-firewall-rules-optional","title":"Modify Firewall Rules (Optional)","text":"<p>This is only required in a two-system setup with a Windows OEM system.  Depending on the network setup, it may be necessary to create an inbound rule in order to allow traffic in for SCP on port 2222 and for the port forwarding on port 2223. Creating inbound/outbound rules on Windows can be done following the instructions here. To turn off the Windows Defender Firewall, follow the steps here.</p>"},{"location":"getting-started/choose-config.html#configure-specific-services","title":"Configure Specific Services","text":"<p>If a custom configuration option is not needed, return to the main installation flow, Build and Deploy. </p> <p>The table below provides links to configuration options along with the computer it should run on (if using a two system setup). </p> Component Description Modify Configuration To File Watcher The File Watcher component monitors selected folder(s) for new files to process. Exercise greater control over the File Watcher component. (e.g., Refinement of file filtering) Data Organizer &gt; Attribute Parser The Data Organizer helps route calls from other microservices to the job repository. The attribute parser of the data organizer can parse the file name of a job for attributes. Customize file name information gathering. Task Launcher &gt; RetryWindow  The Task Manager manages and launches tasks for jobs to be executed on the Pipeline Simulator, Geti pipelines, or BentoML pipelines Set the frequency for resubmitting jobs to a pipeline."},{"location":"getting-started/choose-config.html#next-up","title":"Next up","text":"<p>Build and Deploy</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"getting-started/install-prereqs.html","title":"Setup Prerequisites on OEM and Gateway Systems","text":""},{"location":"getting-started/install-prereqs.html#setup-wsl2-for-windows-oem-system","title":"Setup WSL2 for Windows OEM system","text":"<ol> <li>Install WSL using the Windows store</li> <li>In Powershell, set WSL to version 2     <pre><code>wsl --set-default-version 2\n</code></pre></li> <li>Install the following tools in WSL2 following the steps under Install Prereqs<ul> <li>git</li> <li>make</li> <li>curl</li> <li>Docker Engine</li> </ul> </li> <li> <p>Edit <code>/etc/wsl.conf</code></p> <ul> <li>Enable systemd <pre><code>[boot] systemd = true\n</code></pre></li> <li>Disable generate hosts <pre><code>[network] generateHosts = false\n</code></pre></li> </ul> </li> <li> <p>To get the software on the OEM device, clone the AiCSD code repository using one of the options below using WSL terminal:</p> HTTP <pre><code>git clone https://github.com/intel/AiCSD.git\n</code></pre> SSH <pre><code>git clone git@github.com:intel/AiCSD.git\n</code></pre> <p>Note</p> <p>To update to a specific version of the software after cloning, use:</p> <pre><code>git checkout &lt;version-tag&gt;\n</code></pre> </li> <li> <p>Setup WSL2 Port Forwarding within OEM device</p> <ul> <li>Using Admin PowerShell on OEM Device, navigate to the AiCSD code directory <pre><code>cd \\\\wsl.localhost\\Ubuntu\\...\\AiCSD\n</code></pre></li> <li>Run the <code>WSL_Port_Setup_Script.ps1</code> PowerShell script.     <pre><code>.\\WSL_Port_Setup_Script.ps1\n</code></pre></li> </ul> <p>Note</p> <p>To remove the port forwarding rules, using Admin PowerShell, run the <code>WSL_Port_Removal_Script.ps1</code> PowerShell script. <pre><code>.\\WSL_Port_Removal_Script.ps1\n</code></pre></p> </li> </ol>"},{"location":"getting-started/install-prereqs.html#setup-ubuntu-for-linux-oem-or-gateway-system","title":"Setup Ubuntu for Linux OEM or Gateway System","text":"<p>The only required setup for these systems is to install Ubuntu 20.04.</p>"},{"location":"getting-started/install-prereqs.html#install-prerequisites","title":"Install Prerequisites","text":"<p>Install make, git, curl, Docker, and Docker Compose.</p> <ol> <li>Update repositories:     <pre><code>sudo apt-get update\n</code></pre></li> <li>Install make, git, and curl:    <pre><code>sudo apt-get install -y make git curl\n</code></pre></li> <li> <p>Install Docker Engine.</p> <p>Note</p> <p>Only follow the steps for Install Using the Repository in the docker setup guide linked above. This will set up the repository and install the Docker Engine and Docker Compose, which are necessary for setting up the microservices.</p> </li> <li> <p>Follow these post-installation steps to Manage Docker as a non-root user.</p> </li> <li> <p>Ensure access and configurations to be able to clone the AiCSD code repository using either SSH or HTTPS. For SSH, refer to Generating a new SSH key and add the public key to GitHub profile account settings. For HTTPS, refer to Creating a Personal Access Token for authentication purposes. Treat the key as a password.</p> </li> </ol>"},{"location":"getting-started/install-prereqs.html#download-repository","title":"Download Repository","text":"<ol> <li> <p>To get the software, clone the AiCSD code repository using one of the options below:</p> HTTP <pre><code>git clone https://github.com/intel/AiCSD.git\n</code></pre> SSH <pre><code>git clone git@github.com:intel/AiCSD.git\n</code></pre> <p>Note</p> <p>To update to a specific version of the software after cloning, use:</p> <pre><code>git checkout &lt;version-tag&gt;\n</code></pre> </li> <li> <p>Navigate to the working directory:</p> <pre><code>cd AiCSD\n</code></pre> </li> </ol>"},{"location":"getting-started/install-prereqs.html#next-up","title":"Next up","text":"<p>Gateway Installation &gt; Configure</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"getting-started/ms-web-ui-guide-dashboards.html","title":"Web User Guide - Dashboard","text":"<p>The dashboard provides additional metrics and monitoring for the system. Currently, the software uses Grafana, the open-source monitoring and analytics platform, to display metrics and monitoring in a dashboard.</p> <p>Note</p> <p>To view the Grafana UI, start the monitoring tech stack as described in Monitoring Overview.</p> <p>To use the dashboard: </p> <p>Navigate to the Dashboards page by clicking on the DASHBOARDS tab in the tab strip at the top.</p> Figure 1: Display Grafana UI <p>This page will open a new tab to the Grafana UI default dashboard as shown in Figure 2.</p> Figure 2: Display Grafana Dashboard"},{"location":"getting-started/ms-web-ui-guide-dashboards.html#next-up","title":"Next up","text":"<p>Troubleshooting</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"getting-started/ms-web-ui-guide-jobs.html","title":"Web User Guide - Jobs","text":""},{"location":"getting-started/ms-web-ui-guide-jobs.html#display-all-jobs","title":"Display All Jobs","text":"<p>To display all jobs: </p> <p>Navigate to the Job Management page by clicking VIEW JOBS.</p> Figure 1: Display All Jobs <p>Note</p> <p>Jobs are updated automatically every 10 seconds.</p>"},{"location":"getting-started/ms-web-ui-guide-jobs.html#display-job-details","title":"Display Job Details","text":"<p>To see job details:</p> <ul> <li>Click on the down carrot in the first column to display the details about that particular input file.</li> <li>Click on Expand All Input File Details button to view the information about all input files at once.</li> </ul> Figure 2: Display Filename Jobs"},{"location":"getting-started/ms-web-ui-guide-jobs.html#display-job-output-file-information","title":"Display Job Output File Information","text":"<p>To view the output file(s):</p> <p>Click on the button in the output file column. A box will appear with all the associated  output files. </p> Figure 3: Display Output Files Jobs <p>To view the output file(s) status:</p> <p>Hover the mouse over the File Status for the desired file as shown in Figure 4. </p> Figure 4: Output File Status Tool Tip <p>If the status is an error, hovering the mouse over the Output File Path will show the error details, as shown in Figure 5.</p> Figure 5: Output File Error Details Tool Tip"},{"location":"getting-started/ms-web-ui-guide-jobs.html#navigating-job-information","title":"Navigating Job Information","text":"<p>To control how jobs are displayed:</p> <ul> <li>Enter text in the text box to filter jobs. In this example, the jobs with errors associated with them are displayed.</li> <li>Control the number of jobs on the page using pagination provided at the bottom of the page.</li> <li>Click on individual columns to sort jobs either in ascending or descending order.</li> </ul> Figure 6: Display Job Error Details"},{"location":"getting-started/ms-web-ui-guide-jobs.html#next-up","title":"Next up","text":"<p>Web UI Guide - Model Upload</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"getting-started/ms-web-ui-guide-tasks.html","title":"Web User Guide - Task Management","text":""},{"location":"getting-started/ms-web-ui-guide-tasks.html#display-all-tasks","title":"Display All Tasks","text":"<p>The default landing page is Task Management. Clicking the Create/Modify Tasks at the top-left of the navigation bar also displays the Task Management page. This page will display all the tasks currently available in the system.</p> Figure 1: Display All Tasks <p>The UI, shown in Figure 1, offers different ways to look at the current tasks:</p> <ol> <li> <p>Filter: enter text in the textbox to filter tasks.</p> </li> <li> <p>Sort: click on individual columns to sort tasks either in ascending or descending order.</p> </li> <li> <p>View pages: use pagination, provided at the bottom of the page, to display a set number of tasks on the page.</p> </li> </ol>"},{"location":"getting-started/ms-web-ui-guide-tasks.html#create-task","title":"Create Task","text":"<p>To create a task: </p> <ol> <li> <p>From Task Management page, select Add Task. </p> <p> Figure 2: Create Task </p> </li> <li> <p>Complete the following fields with the task details:</p> <p>Note</p> <p>Regular expressions are not supported.</p> <p>For example, in Job Selector below, use of contains with \"*.tiff\" will filter for tasks with the literal substring \"*.tiff\".</p> <ul> <li> <p>Description: enter Task Description</p> </li> <li> <p>Pipeline: click to select the Model Execution Pipeline from the dropdown</p> </li> <li> <p>Job Selector: click to select between the two checkboxes -</p> <p>--  matches: filter jobs based on filename that exactly match \u201cfilename\u201d field value from below</p> <p>--  contains: filter jobs based on filename that contain \u201cfilename\u201d field value from below</p> </li> <li> <p>Filename: enter input image filename   </p> </li> <li> <p>Model Parameters: enter parameter details for the model, it should adhere to the following json format, including quotes \u2013 {\u201cparameter\u201d : \u201dvalue\u201d}</p> </li> </ul> <p> Figure 3: Task Form </p> </li> <li> <p>Click Save.</p> </li> </ol>"},{"location":"getting-started/ms-web-ui-guide-tasks.html#update-task","title":"Update Task","text":"<ol> <li> <p>Click on Update for the task that needs to be modified:</p> <p> Figure 4: Update Task </p> </li> <li> <p>Modify the appropriate field (i.e., Description modified)</p> <p> Figure 5: Modify Field Task </p> </li> <li> <p>Click Save.</p> </li> <li> <p>Confirm that the changes are saved as shown below:</p> <p> Figure 6: Updated Task </p> </li> </ol>"},{"location":"getting-started/ms-web-ui-guide-tasks.html#delete-task","title":"Delete Task","text":"<ol> <li>Select one or more checkboxes next to the entry/entries to delete.</li> <li>Click Delete Selected. </li> <li> <p>Confirm the deletion:</p> <p> Figure 7: Task Deletion </p> </li> </ol>"},{"location":"getting-started/ms-web-ui-guide-tasks.html#next-up","title":"Next up","text":"<p>Web UI Guide - Jobs</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"getting-started/ms-web-ui-guide-upload-model.html","title":"Web User Guide - Model Upload","text":"<p>The Upload Models page provides functionality to upload the AI/ML models to create new pipelines. This feature supports Intel Geti models and those served by the OpenVINO\u2122 Model Server (OVMS)</p> <p>To upload a new model:</p> <ol> <li> <p>Complete the following fields with the model details:</p> <ul> <li> <p>Model Name: enter Model Name</p> </li> <li> <p>Model Type: click to select the type of the model - </p> <p>-- Geti</p> <p>-- OpenVino Model Server(OVMS) - default option</p> </li> </ul> </li> <li> <p>Click Browse to select the zipped Geti/Ovms models.</p> <p>Warning</p> <ul> <li>Zip file is required. </li> <li>Correct Geti/OVMS directory structure must be followed.</li> </ul> </li> <li> <p>Click Save.</p> </li> </ol> Figure 1: Upload Model"},{"location":"getting-started/ms-web-ui-guide-upload-model.html#next-up","title":"Next up","text":"<p>Web UI Guide - Dashboards</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"getting-started/ms-web-ui-guide.html","title":"Web User Guide","text":"<p>This guide provides a description of the UI and instructions for managing tasks and jobs.</p>"},{"location":"getting-started/ms-web-ui-guide.html#prerequisites","title":"Prerequisites","text":"<p>For successful operation, ensure that all microservices are running. If Portainer is up, you can verify services here.</p> <p>Note</p> <p>If running distributed services (i.e., 2-system configuration of Linux Edgebox and WSL2/Linux OEM system), </p> <p>then use the IP address rather than localhost or the URL http://127.0.0.1.</p> Figure 1: Portainer Container List"},{"location":"getting-started/ms-web-ui-guide.html#interface-basics","title":"Interface Basics","text":"<p>The landing page shown in Figure 2 opens to the Task Management tab as the default. The UI enables management of: </p> <ul> <li> <p>Tasks define how input files are matched to a processing pipeline.</p> </li> <li> <p>Jobs define the input file and track the status of the input file as it is processed. Jobs also track the results   and/or output files for the given input file. </p> </li> <li> <p>Analytics are gathered and displayed using Grafana.</p> </li> <li> <p>Models define a processing pipeline.</p> </li> </ul> <p>Figure 2 presents:</p> <ol> <li> <p>Theme Toggle: control the visual theme</p> </li> <li> <p>Create/Modify Tasks: choose this option to view and manage Tasks</p> </li> <li> <p>View Jobs: choose this option to view and manage Jobs</p> </li> <li> <p>Dashboards: choose this option to view and manage analytics</p> </li> <li> <p>Upload Models: choose this option to upload a model for use by a Task</p> </li> </ol> Figure 2: Landing Page"},{"location":"getting-started/ms-web-ui-guide.html#access","title":"Access","text":"<p>To access the Web UI, click here.</p> <p>Note</p> <p>If running distributed services (i.e., 2-system configuration of Linux Edgebox and WSL2/Linux OEM system), use the IP address rather than localhost or the URL http://127.0.0.1.</p>"},{"location":"getting-started/ms-web-ui-guide.html#set-theme","title":"Set Theme","text":"<p>There are two themes, illustrated with the Figures below:</p> Figure 3: Light Theme Figure 4: Dark Theme <p>Toggle between the two themes by clicking the sun-moon button as shown in Figures 3 and 4.</p>"},{"location":"getting-started/ms-web-ui-guide.html#next-up","title":"Next up","text":"<ul> <li>Learn more about Tasks</li> <li>Learn more about Jobs</li> </ul>"},{"location":"getting-started/system-setup.html","title":"Build and Deploy","text":""},{"location":"getting-started/system-setup.html#build-containers","title":"Build Containers","text":"<ol> <li> <p>Use one of the command line options below to build: </p> Build Type Build Command When To Use General Build <code>make &lt;docker-target&gt;</code> The system resources are unknown. Fast Build <code>make -j&lt;num of threads&gt; &lt;docker-target&gt;</code> The flag -j represents the number of jobs in a build. The optimal integer for j depends on the system resources (e.g., cores) and configuration. <p>Where <code>&lt;docker-target&gt;</code> can be one of the following:</p> Docker Target Description docker builds all custom docker containers to run in a single system setup docker-build-gateway builds gateway custom docker containers docker-build-oem builds oem specific custom docker containers - this requires the SSH Configuration to be complete before the containers can be built <p>Warning</p> <p>Leave plenty of time for this step to complete, up to 30-40 minutes. Console output may appear to hang while conducting parts of the build process using intermediate containers. Red text is not necessarily concerning, so allow the process to finish.</p> </li> <li> <p>For a two system setup, on the Gateway, it is necessary to update <code>&lt;OEM_IP_ADDRESS&gt;</code> with the appropriate IP address of the OEM system in the docker-compose-edgex-spiffe-spire.yml file. To obtain the IP address use <code>hostname -I</code> on Linux or <code>ipconfig</code> on Windows. </p> <p>Note</p> <p>For an OEM system running in WSL, use the IP address of the Windows system.</p> </li> </ol>"},{"location":"getting-started/system-setup.html#run-the-services","title":"Run the Services","text":"<p>The table below describes the different run options available. </p> <ol> <li> <p>Run the docker images for the gateway (or all on one, if desired). Choose one of the run targets below (depends on the pipeline type and number of systems):</p> <p>Note</p> <p>It is recommended to use the simulated pipeline for a trial setup. Using the simulated pipeline on a single system is the simplest configuration.</p> Pipeline Option Run Target Description Number of Containers Simulated Pipeline (Gateway) <code>make run-gateway-sim</code> Run the AiCSD microservices with a set of simulated pipelines 22 Simulated Pipeline (single system) <code>make run-sim</code> Run the AiCSD microservices with a set of simulated pipelines 21 Geti Pipeline (Gateway) <code>make run-gateway-geti</code> Run the AiCSD microservices with Geti pipelines for the Gateway System 23 Geti Pipeline (single system) <code>make run-geti</code> Run the AiCSD microservices with Geti pipelines 22 OpenVino Model Server (Gateway or single) <code>make run-ovms</code> Run the OVMS container 1 <p>Note</p> <p>The optional <code>GATEWAY_IP_ADDR=192.168.X.X</code> parameter can be added to the make command in order for the web UI to be accessed fom an external system. This command would read <code>make &lt;run-option&gt; GATEWAY_IP_ADDR=192.168.X.X</code>. To get the IP address of a Linux system run <code>hostname -I</code>.</p> <p>Success</p> <p>For two system, verify that the logs of the <code>edgex-oem-ssh-proxy</code> container say <pre><code>   + scp -p -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' -P 2223 /srv/spiffe/remote-agent/agent.key '&lt;OEM_IP_ADDRESS&gt;:/srv/spiffe/remote-agent/agent.key'\n   scp: Connection closed\n   ssh: Could not resolve hostname &lt;oem_ip_address&gt;: Try again \n</code></pre></p> <p>Success</p> <p>For an AiCSD microservice, the logs should look something like this task-launcher sample log: <pre><code>   level=INFO ts=2023-06-12T16:58:55.042728315Z app=app-task-launcher source=server.go:162 msg=\"Starting HTTP Web Server on address task-launcher:59785\"\n   level=INFO ts=2023-06-12T16:58:55.043443755Z app=app-task-launcher source=messaging.go:104 msg=\"Subscribing to topic(s): 'NONE' @ redis://edgex-redis:6379\"\n   level=INFO ts=2023-06-12T16:58:55.043454019Z app=app-task-launcher source=messaging.go:113 msg=\"Publishing to topic: '{publish-topic}' @ redis://edgex-redis:6379\"\n   level=INFO ts=2023-06-12T16:58:55.04346657Z app=app-task-launcher source=service.go:202 msg=\"StoreAndForward disabled. Not running retry loop.\"\n   level=INFO ts=2023-06-12T16:58:55.043471401Z app=app-task-launcher source=service.go:205 msg=\"Started the task launcher microservice\"\n   level=INFO ts=2023-06-12T16:58:55.043518377Z app=app-task-launcher source=messaging.go:125 msg=\"Waiting for messages from the MessageBus on the 'NONE' topic\"\n   level=DEBUG ts=2023-06-12T17:28:54.018134531Z app=app-task-launcher source=secrets.go:345 msg=\"token is successfully renewed\"\n</code></pre></p> </li> <li> <p>For a two system setup, on the Gateway, add the server entries for the OEM side to authorize the services running on the OEM side.</p> <pre><code>  $ make add-ssh-server-entry\n</code></pre> <p>Note</p> <p>This only needs to be run the first time the Gateway services are started or after the volumes have been cleaned.</p> </li> <li> <p>For a two system setup, start the OEM services on the OEM system. (This should start 6 containers.) </p> <pre><code>  $ make run-oem\n</code></pre> <p>Warning</p> <p>The OEM system must be started within one hour of starting the Gateway system.  Failure to do so will result in the services not connecting or functioning properly.</p> <p>Success</p> <p>The <code>edgex-remote-spire-agent</code> logs should have lines that look like: <pre><code>    time=\"2023-06-05T22:20:35Z\" level=debug msg=\"Fetched X.509 SVID\" count=1 method=FetchX509SVID pid=3559771 registered=true service=WorkloadAPI spiffe_id=\"spiffe://edgexfoundry.org/service/app-file-receiver-oem\" subsystem_name=endpoints ttl=3597.506151713\n</code></pre> An AiCSD microservice (ie <code>file-receiver-oem</code>) should look like: <pre><code>    level=INFO ts=2023-06-07T15:31:23.916977876Z app=app-file-receiver-gateway source=server.go:162 msg=\"Starting HTTP Web Server on address file-receiver-gateway:59783\"\n</code></pre></p> </li> <li> <p>Verify the correct number of containers are running using <code>docker ps</code> or <code>make run-portainer</code>. If using Portainer, open Portainer in a browser. Here's an example screenshot of all the gateway services running in Portainer (high-level check that the stack is in a green/running state):</p> <p></p> </li> </ol>"},{"location":"getting-started/system-setup.html#next-up","title":"Next Up","text":"<p>For Basic Workflow with Simulated Pipelines, no pipeline configuration is needed.</p> <p>Now, continue to the Basic Workflow page.</p>"},{"location":"getting-started/system-setup.html#advanced-workflows","title":"Advanced Workflows","text":"<p>The following are Advanced Workflows for building custom pipelines. </p> <p>Note</p> <p>It is recommended to first get familiarized using the Basic Workflow with Simulated Pipelines before attempting the Advanced Workflows.</p> Run Option Pipeline Configuration Instructions Custom Pipeline with OVMS &amp; BentoML Image Classification Demo Geti Pipeline Use Geti Pipelines <p>Note</p> <p>To create custom pipelines that interfaces with AiCSD, refer to the Pipeline Creation section.</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"getting-started/tear-down.html","title":"Stop Services and Clean Up","text":"<p>The section details how to stop all the software services and clean up the environments. It can also be useful when Troubleshooting.</p>"},{"location":"getting-started/tear-down.html#stop-services-on-gateway-or-single-system","title":"Stop Services on Gateway (or single system)","text":"<p>To stop the services, run</p> <pre><code>make down\n</code></pre> <p>This will bring down all services that are running.</p>"},{"location":"getting-started/tear-down.html#stop-services-on-oem","title":"Stop Services on OEM","text":"<p>To stop the services, run</p> <pre><code>make down-oem\n</code></pre> <p>This will bring down all services that are running.</p>"},{"location":"getting-started/tear-down.html#clean-up-for-any-system","title":"Clean Up for Any System","text":"<ol> <li> <p>To clean up the docker images, run:</p> <p><pre><code>make clean-images\n</code></pre> 2. To clean up the files from the directories on the Linux system, run:</p> <pre><code> make clean-files\n</code></pre> <p>Warning</p> <p>This is a destructive action that will delete input and output files in <code>oem-files</code> and <code>gateway-files</code> folders under <code>$HOME/data/</code> as well as generated secrets files under <code>/tmp/edgex/secrets</code>.</p> </li> <li> <p>To remove any volumes from the system, run:</p> <pre><code>make clean-volumes\n</code></pre> <p>Warning</p> <p>This is a destructive action and will clean up any unused docker volumes. This will delete the database and all its contents when run.</p> <ol> <li>To remove any generated ssh keys from the system, run:</li> </ol> </li> </ol> <pre><code>make clean-keys\n</code></pre> <p>Warning</p> <p>This is a destructive action and will clean up any generated ssh keys. This will mean that the keys need to be regenerated on the Gateway and copied onto the OEM system.</p>"},{"location":"getting-started/troubleshooting.html","title":"Troubleshooting","text":"<p>This guide contains troubleshooting for common issues and errors.</p>"},{"location":"getting-started/troubleshooting.html#build-errors","title":"Build Errors","text":"<p>When building the Docker containers, if the build fails with an error message like:</p> <pre><code>ERROR: failed to solve: golang:1.21.3-alpine3.17: failed to do request: Head \"https://registry-1.docker.io/v2/library/golang/manifests/1.19-alpine3.16\": dial tcp: lookup registry-1.docker.io on 127.0.0.53:53: read udp 127.0.0.1:35314-&gt;127.0.0.53:53: i/o timeout\nmake: *** [Makefile:106: docker-job-repository] Error 1\n</code></pre> <p>there could be an issue with the system's Docker download rate limit.</p>"},{"location":"getting-started/troubleshooting.html#job-status","title":"Job Status","text":"<p>The table below lists the status and error messages associated with jobs. </p> Job Status Description Next Steps Complete Job has been successfully processed. No next steps. Incomplete Job is still being processed. Check the logs of the container that correspond with the service owner of the job. If a container has stopped running, restart that container. Then call the retry endpoint for the service. To learn about retry endpoints, see Retry Logic. No Pipeline Found No tasks have a job selector that matches with the provided image. Ensure that the desired task has the appropriate job selector. To modify a task in the Web UI, follow the instructions for Update Task. Pipeline Error The pipeline processing the image failed. Check the job Error Details column or QC flags field for details about the error. See View Jobs in Web UI. Also, check the log for the pipeline container.  If the error is permissions-based, the logs may say <code>permission denied</code> or <code>failed to write file</code>. In this case, on the Ubuntu system, run: <code>make files</code> or <code>sudo make files</code>. Transmission Error The job could not be transferred to the message bus. Check the logs of the service in the Error Details column. See View Jobs in Web UI. If necessary, call the retry endpoint for the appropriate service, restart the pipeline and/or restart the Redis container. To learn about retry endpoints, see Retry Logic. File Error (Input) The input file could not be transferred from the File Sender OEM to the File Receiver Gateway. Check the logs of the File Sender OEM. If the owner of the job is still the File Sender OEM, then call the retry endpoint using instructions in Retry Logic. File Error (Output) The output file(s) could not be transferred from the File Sender Gateway to the File Receiver OEM. To learn how to check the error details for the failing output file(s), See Output File Status Info. Check the logs of the File Receiver OEM and the File Sender Gateway for the job. If the owner of the job is still the File Receiver OEM, call the retry endpoint for the File Sender Gateway and File Receiver OEM using the instructions in Retry Logic."},{"location":"getting-started/troubleshooting.html#view-jobs-in-web-ui","title":"View Jobs in Web UI","text":"<p>To see job-level errors in the Web UI:</p> <ol> <li> <p>Choose the View Jobs tab.      Figure 1: View Jobs Tab </p> </li> <li> <p>See the columns Job Status, Pipeline Status and Error Details, which displays detailed information about the error.</p> </li> <li> <p>Compare the contents of these columns with the Job Status table above. </p> </li> </ol> <p>Example</p> <p>The Error Details column presents with this format: <code>(&lt;service name&gt;): &lt;error&gt;</code>. The <code>&lt;service name&gt;</code> is the service to investigate. The <code>&lt;error&gt;</code> is the cause of the failure.</p> <p>Example: <code>(data-organizer): no tasks could be matched to the input file name</code></p> <p>For information about how to view a container's logs, see Check Logs.</p>"},{"location":"getting-started/troubleshooting.html#output-file-status","title":"Output File Status","text":"<p>The output file status messages are listed in the table below. </p> File Status Description File Complete The file is done processing. File Incomplete The file is still processing. File Invalid The file could not be validated (e.g. hostname mismatch or file not found). File Transmission Failed The file could not be transmitted from the File Sender Gateway to the File Receiver OEM. File Write Failed The file could not be written to the OEM system. File Archival Failed The file on the gateway could not be archived by the File Sender Gateway."},{"location":"getting-started/troubleshooting.html#see-output-file-status-information","title":"See Output File Status Information","text":"<p>For information about the output file status:</p> <ol> <li>Click the View Jobs tab to see the Jobs page for the specific job.</li> <li>Click on the Output Files. The output file view will indicate which files failed.</li> <li>Hover over the file status and file name, as shown in Figures 2 and 3, to see the specific error that occurred.</li> </ol> Figure 2: Output File Status Figure 3: Output File Error Details <p>Check the Error Details column to determine the service that failed to process the file. From there, trace the logs back to see the context of the error. If necessary, restart the affected containers or call the appropriate retry endpoints. See Retry Logic.</p>"},{"location":"getting-started/troubleshooting.html#check-logs","title":"Check Logs","text":"<p>The following sections describe various instructions for checking logs. </p> <p>Note</p> <p>Run the instructions on the correct system, Gateway or OEM.</p>"},{"location":"getting-started/troubleshooting.html#portainer","title":"Portainer","text":"<p>Use Portainer to view the status of AiCSD containers. </p> <p>On the Gateway system:</p> <ol> <li>Open a terminal and run:      <pre><code>make run-portainer\n</code></pre></li> <li> <p>Navigate to Portainer.</p> <p>Note</p> <p>The link to Portainer (above) refers to http://localhost:9000. If viewing from the OEM system, use the IP address instead of localhost.</p> </li> <li> <p>Log in and navigate to the containers view. This will show all the running and stopped containers.      Figure 4: Portainer Container View </p> </li> <li>For the desired container, under the Quick actions column, click the paper icon to view the logs.      Figure 5: Portainer Log Icon </li> <li>This will open the logs for the selected container.      Figure 6: Portainer Log </li> </ol>"},{"location":"getting-started/troubleshooting.html#command-line","title":"Command Line","text":"<p>Use command line instructions to view the status of AiCSD containers.</p> <p>On the Gateway or OEM system:</p> <ol> <li> <p>Open a terminal and run:</p> <p><pre><code>docker ps\n</code></pre> This command lists all the running containers.  Figure 7: Docker Container List  2. Select the CONTAINER ID and use that to run:</p> <pre><code>docker logs &lt;container_id&gt;\n</code></pre> <p>This command displays the logs for the selected container.</p> <p> Figure 8: Docker Logs </p> </li> </ol>"},{"location":"getting-started/troubleshooting.html#export-logs-gateway","title":"Export Logs (Gateway)","text":"<p>To export logs from all services (AiCSD and EdgeX), run: </p> <p><pre><code>make logs\n</code></pre> This creates a zip file called logs.zip.</p>"},{"location":"getting-started/troubleshooting.html#retry-logic","title":"Retry Logic","text":"<p>All services have retry endpoints documented in each individual service. See Retry API Table.</p> <p>A service will process jobs it owns when: </p> <ul> <li>Retry endpoints are called. </li> <li>The service starts up.</li> </ul>"},{"location":"getting-started/troubleshooting.html#retry-api-table","title":"Retry API Table","text":"<p>Each service has its own retry endpoint that is shown in the API documentation for that service</p> Microservice Retry Endpoint Port data-organizer 59781 file-sender-oem 59782 file-receiver-gateway 59783 task-launcher 59785 file-sender-gateway 59786 file-receiver-oem 59787"},{"location":"getting-started/troubleshooting.html#retry","title":"Retry","text":"<p>On the Gateway or OEM system, use curl to call the retry endpoint from a Linux terminal:</p> <pre><code>curl -X POST http://&lt;hostname&gt;:&lt;port-number&gt;/api/v1/retry -H 'Content-Type: application/json' -d &lt;payload&gt;\n</code></pre> <p>Note</p> <p>For retry endpoints that do not have a payload, remove the <code>-H 'Content-Type: application/json' -d &lt;payload&gt;</code> from the command above. </p>"},{"location":"getting-started/troubleshooting.html#retry-timeout-window","title":"Retry Timeout Window","text":"<p>The task launcher has a retry timeout window. This window should be greater than the time it takes to process an input file. </p> <p>Change this value in the task-launcher configuration.toml before the service is started or in Consul.</p> <p>If the change is made in the configuration.toml, the task-launcher service must be rebuilt: </p> <ol> <li> <p>Run:</p> <pre><code>make docker-task-launcher\n</code></pre> </li> <li> <p>Restart the service.  </p> </li> </ol> <p>If the change is made in Consul, the task-launcher service must be restarted.</p>"},{"location":"getting-started/troubleshooting.html#security","title":"Security","text":""},{"location":"getting-started/troubleshooting.html#regenerating-ssh-keys-two-system-only","title":"Regenerating SSH Keys (Two System Only)","text":"<p>Any time the ssh keys are regenerated on a two system setup, the public key must be copied back from the Gateway System to the OEM system. Once the key is copied over tho the OEM, the OEM services must be rebuilt in order to have a copy of the new key. </p>"},{"location":"getting-started/troubleshooting.html#consul","title":"Consul","text":"<p>With the implementation of the new security features, Consul requires a security token for user authentication.</p> <ol> <li>Open Consul in a browser.</li> <li>Generate a token to access Consul and copy it.     <pre><code>    $ make get-consul-acl-token\n</code></pre></li> <li>Click <code>Log in with a different token</code> in Consul and paste the generated token. </li> <li>Once authenticated in Consul, it is possible to change configurations for any of the AiCSD services. </li> <li>For any settings (apart from Writable settings), the affected service must be restarted in order for the change to be applied.</li> </ol>"},{"location":"getting-started/troubleshooting.html#server-misbehaving-two-system-setup","title":"Server Misbehaving (Two System Setup)","text":"<p>If an error in the logs appears to be something like</p> <pre><code>Post \\\"https://&lt;system_name&gt;:8443/job-repository/api/v1/job\\\": dial tcp: lookup &lt;system_name&gt; on 127.0.0.11:53: server misbehaving\"\n</code></pre> <p>it may be necessary to stop the services, clean-up, and ensure that the hostname can be resolved properly. </p> <p>If the hostname is not set properly, it is possible to modify the <code>/etc/hosts</code> file to add the ip address and server name.</p>"},{"location":"getting-started/troubleshooting.html#oem-services-hang","title":"OEM Services Hang","text":"<p>If any of the OEM services (ie app-file-watcher) have an error that looks like:</p> <pre><code>level=INFO ts=2023-06-13T19:10:25.315559848Z app=app-file-watcher source=methods.go:138 msg=\"using Unix Domain Socket at unix:///tmp/edgex/secrets/spiffe/public/api.sock\"\n</code></pre> <p>then the services may have hung.</p> <p>Here are some potential fixes for this issue:</p> <ul> <li>Check that systems are on the same network by verifying that the Gateway system can ping the IP address of the OEM system.</li> <li>Check that the public key <code>edgex-res/id_rsa.pub</code> on the Gateway and <code>edgex-res/remote/sshd-remote/authorized_keys</code> on the OEM file are the same.</li> <li>Verify that the <code>TUNNEL_HOST: &lt;OEM_IP_ADDRESS&gt;</code> in the docker-compose-edgex-spiffe-spire.yml address matches the OEM system.</li> <li>Ensure that the logs for both the <code>oem-ssh-proxy</code> on the Gateway and the <code>remote-spire-agent</code> on the OEM have the appropriate logs as described in System Setup </li> </ul>"},{"location":"getting-started/troubleshooting.html#golang-tool-issues","title":"Golang Tool Issues","text":"<p>If you try to run the golang tools like the <code>make auto-verify-clients</code> or <code>make client-update</code> and run into <code>mockery: command not found</code>, there is most likely an issue with environment variables. Similar issue - <code>go-test-report: not found</code> can occur when running <code>make test-report</code>.</p> <p>The fix can be found below:</p> <p>Note</p> <p>You will need sudo privileges to implement this fix.</p> <ol> <li> <p>At the bottom of the <code>$HOME/.bashrc</code> file,  </p> <p>Note</p> <p>You can edit the file with  <pre><code>sudo nano $HOME/.bashrc\n</code></pre></p> <p>Add the following commands:</p> <pre><code>export GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\nexport PATH=$GOPATH/bin:$GOROOT/bin:$PATH:$HOME/bin\n</code></pre> </li> <li> <p>Then run the following command to reload the terminal and set the environment variables     <pre><code>source $HOME/.bashrc\n</code></pre></p> </li> <li> <p>Try running <code>make auto-verify-clients</code> or <code>make client-update</code> or <code>make test-report</code> again.</p> </li> </ol>"},{"location":"getting-started/troubleshooting.html#dashboards","title":"Dashboards","text":"Error or Issue Possible Solution Dashboards Tab: <code>This site can't be reached</code> and <code>localhost refused to connect.</code> To make sure monitoring is properly set up, refer to the Monitoring Overview. For further issues, refer to the documentation above to Check Logs. Other related issues If experiencing additional dashboard issues, refer to the documentation in the Tools Troubleshooting."},{"location":"getting-started/troubleshooting.html#next-up","title":"Next up","text":"<p>To understand how images are processed, see General Information</p> <p>To stop and clean up the services, see Tear Down</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"getting-started/what-to-expect.html","title":"What to Expect","text":"<p>This guide details setup for two working models: - Two System: a Linux Gateway and Windows WSL2 or Linux OEM where the two systems communicate to each other using a secure SSH Tunnel (port forwarding) - Single System: a Linux or Windows WSL2 setup where all services for the OEM and Gateway will run on the same system, likely for development or test purposes. For more information on the services that will be running and how the two systems will communicate, see Image Processing.</p>"},{"location":"getting-started/what-to-expect.html#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/what-to-expect.html#what-youll-need","title":"What You'll Need","text":""},{"location":"getting-started/what-to-expect.html#hardware","title":"Hardware","text":"<p>Configure a network that includes:</p> <ul> <li>System running Ubuntu 20.04 (Gateway)</li> <li>System running Windows\u00ae 10 with WSL2 or Ubuntu 20.04 (OEM)</li> </ul> <p>Both systems must be on the same network.</p>"},{"location":"getting-started/what-to-expect.html#software","title":"Software","text":"<p>The sections under Get Started detail how to install software prerequisites for both systems:</p> <ul> <li>Docker (v &gt;= 23.0) and Docker Compose</li> <li>Make</li> <li>Git</li> <li>Curl</li> </ul>"},{"location":"getting-started/what-to-expect.html#what-youll-do","title":"What You'll Do","text":"<p>Tip</p> <p>** Order of Installation **</p> <p>The order of the steps in Get Started is recommended for first time installation. </p>"},{"location":"getting-started/what-to-expect.html#next-up","title":"Next up","text":"<p>Set Up</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"log-analytics/elasticsearch.html","title":"Elasticsearch","text":""},{"location":"log-analytics/elasticsearch.html#overview","title":"Overview","text":"<p>A free, distributed search and analytics engine, Elasticsearch offers log and business analytics, full-text search, and security intelligence. The commonly used log analytics technology stack contains Elasticsearch, Logstash, and Kibana (ELK). ELK aggregates logs, performs analysis, and creates visualizations. AiCSD uses the Elasticsearch portion of the ELK stack for indexing the service container logs.</p>"},{"location":"log-analytics/elasticsearch.html#elasticsearch-authentication","title":"Elasticsearch Authentication","text":"<p>Authentication is currently enabled for Elasticsearch. The Elastic Stack security features provide built-in user credentials to help bootstrap the stack setup. Elasticsearch users are initialized with the values of the passwords defined in the <code>.env</code> file.</p>"},{"location":"log-analytics/elasticsearch.html#resources","title":"Resources","text":"<p>It isn't necessary to download Elasticsearch separately as it is included in the ELK stack log analytics feature of AiCSD. However, to find out more about Elasticsearch, see the resources below:</p> <ul> <li>Elasticsearch</li> <li>Elasticsearch Docs</li> </ul> <p>BSD 3-Clause License: See License.</p>"},{"location":"log-analytics/filebeat.html","title":"Filebeat","text":""},{"location":"log-analytics/filebeat.html#overview","title":"Overview","text":"<p>Filebeat is an open-source analytics and interactive visualization web application. AiCSD leverages Kibana UI to display container and service logs captured by Filebeat and forwarded to Elasticsearch.</p> <p>For more information on Filebeat, please refer to the official documentation here.</p>"},{"location":"log-analytics/filebeat.html#filebeat-authentication","title":"Filebeat Authentication","text":"<p>Authentication is currently enabled for Filebeat. The Elasticsearch UI supports a <code>beats_system</code> user created by the bootstrapping of the ELK stack setup. The <code>beats_system</code> user is one of which will be used by Filebeat to forward logs to Elasticsearch.</p>"},{"location":"log-analytics/filebeat.html#filebeat-over-logstash","title":"Filebeat Over Logstash","text":"<p>AiCSD implements a variation of the popular log analytics stack containing Elasticsearch, Logstash, and Kibana (ELK). It replaces Logstash with Filebeat, eliminating Logstash's JVM installation requirement. In addition, Filebeat uses fewer resources on the installation machine. </p>"},{"location":"log-analytics/filebeat.html#resources","title":"Resources","text":"<ul> <li>General Information: Filebeat</li> <li>Filebeat Docs</li> </ul> <p>BSD 3-Clause License: See License</p>"},{"location":"log-analytics/kibana.html","title":"Kibana","text":""},{"location":"log-analytics/kibana.html#overview","title":"Overview","text":"<p>Kibana is an open-source data visualization and exploration tool for reviewing logs and events. AiCSD uses Kibana to analyze and visualize container and service logs in an easy-to-use UI.</p>"},{"location":"log-analytics/kibana.html#visit-kibana-ui","title":"Visit Kibana UI","text":"<p>Navigate to http://localhost:5601</p>"},{"location":"log-analytics/kibana.html#kibana-authentication","title":"Kibana Authentication","text":"<p>Authentication is currently enabled for Kibana. To log in, the username is <code>elastic</code> and the password is <code>ELASTIC_PASSWORD</code>. These authentication values were set in the <code>.env</code> file.</p> <p>The Kibana UI creates two users, <code>kibana_system</code> and <code>beat_system</code>, as part of the bootstrapping of the ELK stack setup. These are not used for login purposes. These are service accounts that allow the services to communicate properly and visualize the container and service logs.</p>"},{"location":"log-analytics/kibana.html#resources","title":"Resources","text":"<ul> <li>Kibana.</li> </ul> <p>BSD 3-Clause License: See License.</p>"},{"location":"log-analytics/overview.html","title":"Overview","text":"<p>The open-source tools, Elasticsearch, Logstash, and Kibana (ELK), comprise the ELK stack, a tech stack commonly used for log analytics. AiCSD implements a variation of the ELK tech stack for its container and service logs, replacing Logstash with the use of Filebeat.</p> <p>Figure 1 illustrates the data flow and component relationships in the stack.</p>"},{"location":"log-analytics/overview.html#architecture","title":"Architecture","text":"Figure 1: Log Analytics <p>Note</p> <p>It isn't necessary to download the tools of the ELK stack separately as they are included in the log analytics feature of AiCSD. However, to find out more about each tool, see the Resources section for each of the tools.</p>"},{"location":"log-analytics/overview.html#enable-log-analytics","title":"Enable Log Analytics","text":"<p>To enable log analytics services, update authentication information in the <code>.env</code> file found within the root directory, as described in Run the Services. </p> <p>Note</p> <p>While some out-of-the-box security measures were taken for the single-user environment, this repository contains the base implementation for log analytics. It is not configured for complex projects or deployment-specific security concerns.</p>"},{"location":"log-analytics/overview.html#run-the-services","title":"Run the Services","text":"<p>To update the authentication information in the <code>.env</code> file:</p> <ol> <li>Open up <code>.env</code> found at the root level of this project.</li> <li>Scroll down to the bottom of the file to the <code>Log analytics</code> section.</li> <li>Find the <code>ELASTIC_PASSWORD</code> variable and update the value with a password at least 8 characters in length. Be sure to remove the <code>&lt;&gt;</code> characters.</li> <li>Find the <code>KIBANA_SYSTEM_PASSWORD</code> variable and update the value with a password at least 8 characters in length. Be sure to remove the <code>&lt;&gt;</code> characters.</li> <li> <p>Start Elasticsearch, Filebeat, and Kibana:</p> <pre><code>make run-log-analytics\n</code></pre> <p>This command creates a <code>log-analytics</code> docker network and bring up the services necessary for these purposes.</p> </li> </ol>"},{"location":"log-analytics/overview.html#view-log-analytics","title":"View Log Analytics","text":"<p>To view the log analytics within the Kibana UI, visit: http://localhost:5601.</p> <p>To best navigate the Kibana UI, follow these next steps:</p> <ol> <li> <p>Log in using the <code>elastic</code> username and the <code>ELASTIC_PASSWORD</code> that was set in the <code>.env</code> file.    </p> </li> <li> <p>Go to the nav bar in the upper left-hand corner and click on the <code>Logs</code> section under <code>Observability</code>.    </p> </li> <li> <p>Click in the <code>Search for log entries...</code> section and search the logs using the field of most interest.    A first suggestion would be to type <code>container.name</code>, then click on the field that pops up titled <code>container.name</code>.    </p> </li> <li> <p>Type in the name of one of the services of interest and click <code>Live Stream</code>.    </p> </li> <li> <p>Verify logs are streaming in the entries section below.</p> </li> </ol> <p>BSD 3-Clause License: See License.</p>"},{"location":"monitoring/grafana.html","title":"Grafana","text":""},{"location":"monitoring/grafana.html#overview","title":"Overview","text":"<p>Grafana is an open-source analytics and interactive visualization web application. AiCSD uses Grafana dashboards to display system metrics captured by Telegraf and stored within InfluxDB. </p> <p>The landing page of the Grafana UI displays the default system health dashboard, <code>Telegraf Metrics Dashboard</code>.</p>"},{"location":"monitoring/grafana.html#visit-grafana-ui","title":"Visit Grafana UI","text":"<p>Navigate to http://localhost:3001</p>"},{"location":"monitoring/grafana.html#grafana-authentication","title":"Grafana Authentication","text":"<p>Authentication is currently disabled for Grafana. Access to the dashboards is enabled automatically for a more seamless user experience.</p>"},{"location":"monitoring/grafana.html#dashboard-and-metrics","title":"Dashboard and Metrics","text":"<p>The dashboard displays the system health metrics captured by Telegraf and stored within InfluxDB.</p> <p>This dashboard refreshes every 5 seconds and is preconfigured to use InfluxDB and the <code>systemHealthMonitoring</code> bucket for data. </p> <p>Adjust any of the fields at the top to better customize the dashboard.</p>"},{"location":"monitoring/grafana.html#resources","title":"Resources","text":"<ul> <li>General Information: Grafana</li> <li>Grafana Docs</li> </ul> <p>BSD 3-Clause License: See License.</p>"},{"location":"monitoring/influxdb.html","title":"InfluxDB","text":""},{"location":"monitoring/influxdb.html#overview","title":"Overview","text":"<p>InfluxDB is an open-source time series database that is commonly combined with Telegraf and Grafana in monitoring implementations. InfluxDB is a preconfigured data source for the Grafana UI within AiCSD, which enables the Telegraf system metrics stored within InfluxDB to automatically be displayed using Grafana. InfluxDB is incorporated in the AiCSD monitoring stack through the use of an output plugin specified within the Telegraf configuration. </p> <p>System metrics are collected by Telegraf and stored within an InfluxDB bucket. The <code>systemHealthMonitoring</code> bucket is the InfluxDB storage bucket used for system health metrics. </p>"},{"location":"monitoring/influxdb.html#influxdb-20","title":"InfluxDB 2.0","text":"<p>InfluxDB version 2.0 is used along with Flux-styled queries. Flux is InfluxData's functional data scripting language designed for querying, analyzing, and acting on data. Grafana enables Flux-styled queries to interact with the InfluxDB data for visualizations.</p>"},{"location":"monitoring/influxdb.html#visit-influxdb-ui","title":"Visit InfluxDB UI","text":"<p>Navigate to http://localhost:8086</p>"},{"location":"monitoring/influxdb.html#influxdb-authentication","title":"InfluxDB Authentication","text":"<p>InfluxDB version 2.0 provides bootstrap functionality to enable an initial admin user, organization, and storage bucket. Additional environment variables are used to configure the setup logic found in the configuration file at the root directory of this project. The environment configuration is contained in the <code>.env</code> file.</p> <p>Note</p> <p>For proper authentication, necessary to enable monitoring, change the placeholder values for password and token before starting the services. These are contained in the <code>.env</code> file. </p> <p>Open the <code>.env</code> file and modify the values for <code>DOCKER_INFLUXDB_INIT_PASSWORD</code> and <code>DOCKER_INFLUXDB_INIT_ADMIN_TOKEN</code>. These variables may be found at the bottom of the file under the <code>Monitoring</code> section and contain the placeholder value of <code>&lt;CHANGE_ME!&gt;</code>. </p>"},{"location":"monitoring/influxdb.html#login","title":"Login","text":"<p>The initial username is <code>admin</code> by default. The password to login is the password set in the <code>.env</code> file before starting the monitoring services. If necessary, create additional users upon the admin initial login.</p>"},{"location":"monitoring/influxdb.html#view-telegraf-data-bucket","title":"View Telegraf Data Bucket","text":"<p>To view the Telegraf data bucket, use the nav bar on the left.</p> <p>Click Data &gt; Buckets &gt; systemHealthMonitoring.</p>"},{"location":"monitoring/influxdb.html#resources","title":"Resources","text":"<ul> <li>InfluxDB</li> <li>InfluxDB Docs</li> </ul> <p>BSD 3-Clause License: See License.</p>"},{"location":"monitoring/overview.html","title":"Overview","text":"<p>The open-source tools, Telegraf, InfluxDB and Grafana (TIG), comprise the TIG stack, a tech stack commonly used for collecting, storing, and displaying time series data. AiCSD uses the TIG stack in the implementation of the health monitoring feature.</p> <p>Figure 1 illustrates the data flow and component relationships in the TIG stack.</p> Figure 1: Monitoring Architecture <p>Note</p> <p>It isn't necessary to download the tools of the TIG stack separately as they are included in the monitoring feature of AiCSD. However, to find out more about each tool, see the Resources section for each of the tools.</p>"},{"location":"monitoring/overview.html#enable-system-health-monitoring","title":"Enable System Health Monitoring","text":"<p>To enable monitoring services, update authentication information in the <code>.env</code> file found within the root directory, as described in Run the Services. This enables Telegraf to send system metrics to InfluxDB. </p> <p>Note</p> <p>While some out-of-the-box security measures were taken for the single-user environment, this repository contains the base implementation for monitoring. It is not configured for complex projects or deployment-specific security concerns.</p>"},{"location":"monitoring/overview.html#run-the-services","title":"Run the Services","text":"<p>To update the authentication information in the <code>.env</code> file:</p> <ol> <li>Open the <code>.env</code> file found at the root level of this project.</li> <li>Scroll down to the bottom of the file to the <code>Monitoring</code> section.</li> <li>Find the <code>DOCKER_INFLUXDB_INIT_PASSWORD</code> variable and update the value with a password at least 8 characters in length. Be sure to remove the <code>&lt;&gt;</code> characters.</li> <li>Find the <code>DOCKER_INFLUXDB_INIT_ADMIN_TOKEN</code> variable, update the token value and remove the <code>&lt;&gt;</code> characters.</li> <li> <p>Start Telegraf, InfluxDB, and Grafana:</p> <pre><code>  make run-monitor\n</code></pre> <p>This command creates a monitoring Docker network, bringing up the services necessary for monitoring purposes.</p> </li> </ol>"},{"location":"monitoring/overview.html#view-system-health","title":"View System Health","text":"<p>To view the default system health dashboard on the Grafana UI, open a browser to http://localhost:3001.</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"monitoring/telegraf.html","title":"Telegraf","text":""},{"location":"monitoring/telegraf.html#overview","title":"Overview","text":"<p>Telegraf acts as a StatsD agent to collect host system metrics. It includes plugins to collect these metrics, many of which are enabled by default. Telegraf may be deployed to ingest data from multiple input sources and deliver that data to numerous output sources. In AiCSD, Telegraf collects system metrics on utilization (CPU, disk, memory, etc.) and writes data to an InfluxDB output plugin.</p>"},{"location":"monitoring/telegraf.html#telegraf-authentication","title":"Telegraf Authentication","text":"<p>Telegraf authenticates with InfluxDB via the <code>DOCKER_INFLUXDB_INIT_ADMIN_TOKEN</code> value specified in the <code>.env</code> file. The bootstrapping of the <code>DOCKER_INFLUXDB_INIT_ADMIN_TOKEN</code> enables a seamless authentication experience between InfluxDB and Telegraf. Telegraf metrics are then written to InfluxDB. </p>"},{"location":"monitoring/telegraf.html#resources","title":"Resources","text":"<ul> <li>Telegraf</li> <li>GitHub</li> <li>Telegraf Docs</li> </ul> <p>BSD 3-Clause License: See License.</p>"},{"location":"pipelines/pipeline-creation.html","title":"Pipeline Creation","text":"<p>Use the steps and interface descriptions in this section to integrate a custom pipeline implementation in AiCSD.</p>"},{"location":"pipelines/pipeline-creation.html#overview","title":"Overview","text":"<p>The reference implementation provides an example pipeline for pipeline management. To understand the position of the pipeline execution component within the AiCSD architecture, see Figure 1 in the Overview. Integration of a new pipeline involves connecting services and communicating through the interfaces described in the following sections. </p> <p>Communication between the pipeline and the reference implementation occurs through both the EdgeX Message Bus using Redis Pub/Sub and REST calls. The EdgeX Message Bus is implemented using a configurable secure Redis Pub/Sub messaging system.  The current solution also exposes messages published on the EdgeX Message Bus using a non-secure MQTT broker. To share information between services, the reference implementation uses the concept of a Job, a data structure containing metadata about the input file and its related output file(s). </p> <p>Note</p> <p>To learn more about how the reference implementation services work, see Image Processing. For more information about the Tasks and Jobs concept, see Interface Basics.</p> <p>The Table below lists the implementation steps and requirements for pipeline integration. For an illustration of the steps and services involved, see Figure 1. </p> Step Requirement Description Note 1. Serve the Pipeline Information The reference implementation UI issues a REST call to obtain the pipeline information served at a REST endpoint. The custom pipeline must serve from the REST endpoint, <code>api/v1/pipelines</code>. 2. Deploy the Pipeline The Pipeline/Model Manager (ie BentoML) deploys the custom pipelines. Developer Guide for Bentos 3. Trigger (start) the Pipeline The Task Launcher publishes an event to the EdgeX Message Bus on the specified topic to trigger the Custom Pipeline. The Custom Pipeline uses the Job information in the data structure. 4. Process the Files This aspect of the implementation is highly dependent on the model, custom pipeline details, and the overall use case of an implementation. Typically, it generates an output file. 5. Make the Callbacks The custom pipeline sends information to the reference implementation Task Manager via REST. This is a two-step process involving updating the Job Repository and notifying the Task Launcher. Figure 1: Pipeline Integration <p>See the interface details in the following sections.</p>"},{"location":"pipelines/pipeline-creation.html#serve-the-pipeline-information","title":"Serve the Pipeline Information","text":"<p>PORT: 10107</p> <p>ENDPOINT: <code>api/v1/pipelines</code></p> <p>HTTP METHOD: GET</p> <p>DESCRIPTION: provides information that aids in setting up Tasks and assigning Jobs to a particular pipeline</p> <p>BODY: application/json</p> <pre><code>{\n    {\n      \"Id\": string,\n      \"Name\": string,\n      \"Description\": string,\n      \"SubscriptionTopic\": string,\n      \"Status\": string,\n    },\n    {\n      // repeat for each pipeline\n    }\n}\n</code></pre>"},{"location":"pipelines/pipeline-creation.html#start-the-pipeline","title":"Start the Pipeline","text":"<p>PROTOCOL: Redis or MQTT Message Broker</p> <p>TOPIC: specified by the task (should correspond to topic the pipeline subscribes to)</p> <p>DESCRIPTION: provides the Job structure to the pipeline via the EdgeX Message Bus</p> <p>BODY: application/json</p> <pre><code>{\n  \"InputFileLocation\": string,\n  \"OutputFileFolder\": string,\n  \"ModelParams\": map[string]string,\n  // the URLs here will already be specific to the job with the necessary parameters filled in\n  \"JobUpdateUrl\": string,\n  \"PipelineStatusUrl\": string\n}\n</code></pre>"},{"location":"pipelines/pipeline-creation.html#process-the-input-file","title":"Process the Input File","text":"<p>File processing is highly dependent on the model, custom pipeline details, and use case of the implementation.</p>"},{"location":"pipelines/pipeline-creation.html#callbacks-from-the-pipeline","title":"Callbacks from the Pipeline","text":"<p>When the pipeline has completed its operations, it will make two calls to send the following information back to the reference implementation.</p>"},{"location":"pipelines/pipeline-creation.html#job-repository-update","title":"Job Repository Update","text":"<p>ENDPOINT (to call): <code>JobUpdateUrl</code> value from the EdgeX Message Bus payload UI</p> <p>HTTP METHOD: PUT</p> <p>DESCRIPTION: updates the job metadata with the pipeline status, quality control flags, output files, and/or results from the pipeline</p> <p>BODY: application/json     <pre><code>{\n    \"Status\" : \"PipelineComplete\", // or \"PipelineFailed\"\n    \"QCFlags\" : \"Quality Control string, can be empty\", \n    \"OutputFiles\" : \"[\n                      {\n                      \"DirName\": \"/path/to/file\",\n                      \"Name\": \"file_name.extension\",\n                      \"Extension\": \"file extension without the .\",\n                      \"Status\": \"current status - can be empty\",\n                      \"ErrorDetails\": \"detailed error message- can be empty\",\n                      \"Owner\": \"current owner - can be empty\"\n                      },\n                      {\n                      // repeat as necessary\n                      }], \n    \"Results\" : \"any results string to be displayed in the AiCSD UI\"\n}\n</code></pre></p>"},{"location":"pipelines/pipeline-creation.html#task-launcher-pipeline-status","title":"Task Launcher Pipeline Status","text":"<p>Note</p> <p>Notify the reference implementation of the Pipeline Status after calling Job Repository Update.</p> <p>ENDPOINT (to call): <code>PipelineStatusUrl</code> value from the EdgeX Message Bus payload</p> <p>HTTP METHOD: POST</p> <p>CONTENT TYPE: <code>text/plain</code></p> <p>DESCRIPTION: notifies the reference implementation of the Pipeline Status</p> <p>BODY CONTENTS: <code>PipelineComplete</code> or <code>PipelineFailed</code></p> <p>BSD 3-Clause License: See License.</p>"},{"location":"pipelines/bentoml/create-bentos.html","title":"Create Bentos","text":""},{"location":"pipelines/bentoml/create-bentos.html#introduction","title":"Introduction","text":"<p>A bento is a file archive with all the model training source code and APIs defined for serving: saved binary models, data files, Dockerfiles, dependencies, and additional configurations. Everything fits together as a unit and is packaged into a standardized format.</p>"},{"location":"pipelines/bentoml/create-bentos.html#bento-service","title":"Bento Service","text":"<p>Every bento must consists of the following files:</p> <ul> <li> <p>service.py - User defined inference program is converted into an api service using this service.py file</p> </li> <li> <p>bentoml.yaml - This is a configuration file used for building a bento</p> </li> </ul> <p>Learn More: Create Bento Service</p>"},{"location":"pipelines/bentoml/create-bentos.html#build","title":"Build","text":"<p>A Bento can be created with the bentoml build CLI command with a bentofile.yaml build file. </p> <p><pre><code> bentoml build\n</code></pre> When bento is built successfully, bento_image_name:bento_image_tag is displayed as shown in Figure 1</p> Figure 1: Successful BentoML Creation <p>List locally built bentos</p> <pre><code> bentoml list\n</code></pre> <p>Display bento contents</p> <pre><code> cd $(bentoml get &lt;bento_image_name&gt;:latest -o path)\n tree\n</code></pre> <p>Learn More: Build Bento</p> Figure 2: Bento Tree"},{"location":"pipelines/bentoml/create-bentos.html#next-up","title":"Next up","text":"<p>Deploy Bentos</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"pipelines/bentoml/deploy-bentos.html","title":"Deploy Bentos","text":"<p>Bentos can be deployed using the following methods:</p> <ul> <li>Containerize and deploy as custom docker containers </li> <li>Use Yatai to deploy them at scale on Kubernetes</li> <li>Use bentoctl for fast model deployment on any cloud platform</li> </ul> <p>Learn More: Bento Deployment</p>"},{"location":"pipelines/bentoml/deploy-bentos.html#deploy-locally","title":"Deploy Locally","text":"<pre><code> bentoml serve\n</code></pre>"},{"location":"pipelines/bentoml/deploy-bentos.html#deploy-docker-containers","title":"Deploy Docker Containers","text":"<p>Create containers with the bento images</p> <p><pre><code> bentoml containerize &lt;bento_image_name&gt;:latest\n</code></pre> Run the generated docker image</p> <p><pre><code> docker run -p 3000:3000 &lt;bento_image_name&gt;:&lt;bento_image_tag&gt; serve --production\n</code></pre> Deployed docker container can be now viewed via Portainer</p> Figure 1: BentoML Container in Portainer <p>After bento is deployed, service can be accessed via swagger api UI - http://0.0.0.0:3000/</p> Figure 2: BentoML Swagger API UI"},{"location":"pipelines/bentoml/deploy-bentos.html#next-up","title":"Next up","text":"<p>Developer Guide</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"pipelines/bentoml/developer-guide-bentos.html","title":"Developer Guide","text":""},{"location":"pipelines/bentoml/developer-guide-bentos.html#overview","title":"Overview","text":"<p>This Developer Guide will walk you through the steps required to create your own BentoML services and integrate them with the AiCSD Project.</p>"},{"location":"pipelines/bentoml/developer-guide-bentos.html#workflow","title":"Workflow","text":"<p>As a Data Scientist, Machine Learning Engineer, or Software/Solutions Engineer building AI/ML software or solutions, BentoML provides an easy way to create, containerize, and deploy ML pipelines as docker services which can then be integrated with the AiCSD project for automatic transfer of scientific images. These images are captured using OEM devices connected to microscopes, cameras, etc. and are transferred to a higher compute Gateway device. The ML pipelines are executed on the Gateway by applying image processing models and by making meaningful inferences to generate output images or results.</p> Figure 1: ML Pipeline Service Development &amp; Deployment Workflow"},{"location":"pipelines/bentoml/developer-guide-bentos.html#ml-pipeline-creation","title":"ML Pipeline Creation","text":"<ol> <li> <p>Develop the initial code for AI-assisted image processing and inferencing. Test it out locally to ensure results are as expected.</p> </li> <li> <p>Either add the model directly to the AiCSD project's models folder(and update config.json if OpenVINO is used for inferencing) or use the Upload Models feature of the UI. Model directory structure must follow the OVMS structure with a subdirectory named 1 and everything else falling inside this as subdirectories.</p> </li> <li> <p>ML pipelines with no models can also be created. However, make sure to either upload the zipped folder with an empty subdirectory of 1 or add the directory with an empty subdirectory of 1 under the AiCSD project's models folder(and update config.json if OpenVINO is used for inferencing). Name of this directory will appear in the pipeline dropdown while Creating a New Task</p> <p>Warning</p> <ul> <li>If the directory (with or without model) is not uploaded, then the ML pipeline will not be created. It will not be displayed in the pipeline dropdown while Creating a New Task</li> <li>Model directory structure must follow the OVMS guidelines with sub-directory named 1 and everything else falling inside this as files or sub-directories.</li> </ul> </li> <li> <p>For integrating the user defined ML pipeline with AiCSD, the below mentioned files need to be included in the project directory. These files for the sample image classification demo can be used as reference during development. These files are present under the demos folder in the AiCSD project repo.</p> <ul> <li> <p>paho_mqtt_client.py - Start a MQTT client listening to request coming from the AiCSD containers</p> </li> <li> <p>pythonClientConfig.yaml - Configuration file for the ML pipeline, gateway_ip, and service/POST_url should be set by the user with the Gateway IP address</p> </li> <li> <p>helper.py - Functions to create response to be sent back to the AiCSD containers</p> </li> <li> <p>service.py - User defined inference program is converted into a BentoML API service using this file</p> </li> <li> <p>bentofile.yaml - Configuration file for building a BentoML service</p> </li> <li> <p>Makefile - Automate the BentoML CLI commands and MQTT client to build and run bentos and the MQTT client docker containers</p> </li> </ul> </li> <li> <p>Ensure the following for integration with AiCSD</p> <ul> <li> <p>Modify main function - add it as a function that will be called from the BentoML  service.py file and processing results will be sent back as response.</p> </li> <li> <p>Import the user defined ML pipeline as a package inside service.py</p> </li> <li> <p>service.py expects the input in the following json format:</p> </li> </ul> <pre><code>    {\n        \"InputFileLocation\":\"/path/to/input file\",\n        \"OutputFileFolder\":\"/path/to/output files\",\n        \"ModelParams\":{\"model parameters specific to the pipeline\"},\n        \"JobUpdateUrl\":\"REST API endpoint for updating job\",\n        \"PipelineStatusUrl\":\"REST API endpoint for updating pipeline status\", \n        \"GatewayIP\":\"gateway IP address\"\n    }\n</code></pre> <ul> <li> <p>Within the @svc.api inside service.py, make calls to the user defined ML pipeline passing the input and output file paths and then store the response as results. </p> <p>Note</p> <p>When deploying BentoML service as docker containers, the Input/Output file path will be /tmp/files/input and /tmp/files/output which are the paths mapped through volume mount during docker execution. If service is deployed locally then these should be absolute filesystem paths.</p> </li> <li> <p>Import helper.py inside the user defined ML pipeline as a package to create the response. AiCSD expects the response in the following format -</p> </li> </ul> <pre><code>    {\n    \"Status\" : \"PipelineComplete\", // or \"PipelineFailed\"\n    \"QCFlags\" : \"Quality Control string, can be empty\", \n    \"OutputFiles\" : \"[\n        {\n            \"DirName\": \"/path/to/file\",\n            \"Name\": \"file_name.extension\",\n            \"Extension\": \"file extension without the .\",\n            \"Status\": \"current status - can be empty\",\n            \"ErrorDetails\": \"detailed error message- can be empty\",\n            \"Owner\": \"current owner - can be empty\"\n        },\n        {\n            // repeat as necessary\n        }\n    ], \n    \"Results\" : \"any results string to be displayed in the AiCSD UI\"\n    }\n</code></pre> <ul> <li> <p>Refer to Job Repository Update for more details on pipeline callbacks.</p> </li> <li> <p>It is important to follow the order of callbacks within @svc.api inside service.py. Job must be updated first followed by Task.</p> </li> <li> <p>It is mandatory to update the Status field as PipelineComplete or PipelineFailed for reflecting the correct job status on AiCSD UI.</p> </li> <li> <p>Set gateway_ip in pythonClientConfig.yaml file.</p> </li> <li> <p>MQTT client is listening to AiCSD's EdgeX MQTT Broker on topic mqtt-export/pipeline/params.</p> </li> <li> <p>Other MQTT parameters like port can be configured in pythonClientConfig.yaml.</p> </li> <li> <p>ML pipeline bento service configurations, like port etc, can be configured within bentoml.yaml. Thus multiple pipelines can be configured to run on different ports for scalability.</p> </li> <li> <p>Logger() doesn't work inside the BentoML container/terminal logs, use print() for logging purposes. </p> </li> </ul> </li> </ol>"},{"location":"pipelines/bentoml/developer-guide-bentos.html#bentoml-service-creation-deployment","title":"BentoML Service Creation &amp; Deployment","text":"<ol> <li> <p>To avoid any package incompatibility issues, it is highly recommended to install and use conda environment.</p> </li> <li> <p>Create and activate a new conda environment     <pre><code>conda create -n bentoml_env\nconda activate bentoml_env\n</code></pre></p> </li> <li> <p>Change directory to the ML pipeline project eg. demos/image_classification_example</p> </li> <li> <p>Install prerequisites     <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li>Build BentoML services following steps mentioned here <pre><code>bentoml build\n</code></pre></li> <li> <p>Note the bento tags - bento_image_name:bento_image_tag as displayed in Figure 2 Figure 2: Successful BentoML Creation </p> </li> <li> <p>Deploy with commands mentioned here</p> <ul> <li>Deploy Locally</li> </ul> <pre><code> bentoml serve\n</code></pre> <p>or</p> <ul> <li>Deploy Container</li> </ul> <pre><code> bentoml containerize &lt;bento_image_name&gt;:latest\n docker run -it --rm -p 3000:3000 -v ${HOME}/data/gateway-files:/tmp/files &lt;bento_image_name&gt;:&lt;bento_image_tag&gt; serve --production\n</code></pre> </li> <li> <p>Verify if service is deployed by opening Swagger API UI - http://0.0.0.0:3000</p> <p>Service can be tested by providing the following json input</p> <p>Note</p> <p>Both AiCSD Services and the python MQTT client should be running for successful response. Refer to Build &amp; Deploy for more information.</p> <pre><code>   {\n        \"InputFileLocation\":\"/tmp/files/input/1.tiff\",\n        \"OutputFileFolder\":\"/tmp/files/output\",\n        \"ModelParams\":{\"Brightness\":\"0\"},\n        \"JobUpdateUrl\":\"http://job-repository:59784/api/v1/job/pipeline/7c9fdbde-3b56-48af-a405-51d718cd9711/15c40f2d-68d2-4f09-9133-23f101b8b660\",\n        \"PipelineStatusUrl\":\"http://task-launcher:59785/api/v1/pipelineStatus/7c9fdbde-3b56-48af-a405-51d718cd9711/15c40f2d-68d2-4f09-9133-23f101b8b660\", \n        \"GatewayIP\":\"192.168.0.20\"\n    }\n</code></pre> <p> Figure 3: Successful Swagger API Response </p> </li> <li> <p>Verify if container is deployed by checking Portainer</p> </li> </ol>"},{"location":"pipelines/bentoml/developer-guide-bentos.html#end-to-end-ml-pipeline-execution","title":"End to End ML Pipeline Execution","text":"<ol> <li> <p>To avoid any package incompatibility issues, it is highly recommended to install and use the conda environment.</p> </li> <li> <p>Create and activate a new conda environment     <pre><code>conda create -n bentoml_env\nconda activate bentoml_env\n</code></pre></p> </li> <li> <p>Change directory to the AiCSD project root folder on the Gateway.</p> </li> <li> <p>Clean the workspace  <pre><code> make clean-volumes clean-images clean-files\n</code></pre></p> </li> <li> <p>Build &amp; Deploy all AiCSD Containers  <pre><code> make docker\n make run-geti run-ovms\n</code></pre></p> </li> <li> <p>Change directory to the demos/custom pipeline folder in the AiCSD project repo.</p> </li> <li> <p>Install prerequisites     <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>If any existing bento and MQTT client docker containers are running, then stop and remove them using make commands run inside the demos/custom pipeline folder in the AiCSD project repo.  <pre><code> make down BENTO_TAG=&lt;bento_image_name&gt;:&lt;bento_image_tag&gt;\n</code></pre> Old BENTO_TAGs (eg. image_classification:px4jhnqscwuntqls) can be found under Images section in Portainer</p> </li> <li> <p>Set gateway_ip and service/POST_url in pythonClientConfig.yaml file.</p> <p>Note</p> <p>gateway_ip can be obtained using this command: <pre><code> hostname -I\n</code></pre></p> </li> <li> <p>Run make commands inside the demos/custom pipeline folder in the AiCSD project repo.  <pre><code> make build \n make docker-build BENTO_TAG=&lt;bento_image_name&gt;:&lt;bento_image_tag&gt;\n make run BENTO_TAG=&lt;bento_image_name&gt;:&lt;bento_image_tag&gt;\n</code></pre> bento_image_name:bento_image_tag should be copied from the output from <code>make build</code> command.  For more details on these individual bentoml build and run commands refer to Build &amp; Deploy BentoML services </p> </li> <li> <p>Follow steps for Basic Workflow to create a new Task, drop the image file for processing and view the results on the UI.</p> </li> </ol> <p>Refer to Complete Demo: Image Classification</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"pipelines/bentoml/image-classification-demo.html","title":"Image Classification Demo","text":""},{"location":"pipelines/bentoml/image-classification-demo.html#overview","title":"Overview","text":"<p>In this end to end demo, the Image Classification example posted in OpenVINO Github Repo is modified to build this BentoML docker service, then integrated to make this ML pipeline work with the AiCSD project. </p>"},{"location":"pipelines/bentoml/image-classification-demo.html#workflow","title":"Workflow","text":"<ol> <li> <p>Image captured from the OEM device is automatically transferred to the Gateway for ML pipeline processing.</p> </li> <li> <p>This ML pipeline performs image processing by reading all images and their labels specified in the text file. It then classifies them with the ResNet50 model and presents accuracy and latency results.</p> </li> <li> <p>Results are then sent back and can be viewed through the AiCSD UI.</p> </li> </ol>"},{"location":"pipelines/bentoml/image-classification-demo.html#steps-to-run-demo","title":"Steps to Run Demo","text":"<ol> <li> <p>Download the ResNet50 model from OpenVINO Github Repo using the Model Downloader tool from OpenVINO. Follow steps at this link to download the ResNet50 model.</p> </li> <li> <p>For ML pipeline processing with OpenVINO on CPU for the Gateway hardware, select FP32 version of the model. If processing occurs on GPU or VPU, then FP16 version of the model can be used.</p> </li> <li> <p>Downloaded Model consists of the bin &amp; xml files for OpenVINO - resnet50-binary-0001.bin &amp; resnet50-binary-0001.xml.</p> </li> <li> <p>Model directory structure must follow the OpenVINO Model Server guidelines with sub-directory named 1 and everything else falling inside this as files or sub-directories.    Figure 1: OpenVINO Model Server Directory Structure </p> </li> <li> <p>Model Directory sturcture for this resnet model should be as shown below -    Figure 2: Resnet Model Directory Structure </p> </li> <li> <p>Zip the model folder and name it as resnet. </p> <p>Warning</p> <p>Within this zip model, ensure sub-directory is named 1 and everything else falls inside this as files or sub-directories. Upload model in next step will fail if this directory structure is incorrect.</p> </li> <li> <p>Upload this Model from the UI.</p> </li> <li> <p>Image Classification code is available under the demos folder in the AiCSD project repo.</p> </li> <li> <p>Follow the instructions in the End to End ML Pipeline Execution section in the BentoML Developer Guide.</p> </li> <li> <p>Figure 3 displays the job with accuracy and latency as result after executing the image classification ML pipeline via AiCSD.    Figure 3: Image Classification Result displayed in Job UI </p> </li> </ol> <p>BSD 3-Clause License: See License.</p>"},{"location":"pipelines/bentoml/introduction-bentos.html","title":"Overview","text":"<p>BentoML is a Unified Model Serving Framework which makes it easy to create ML-powered prediction services that are ready to deploy and scale. It is used by Data Scientists and ML Engineers to:</p> <ol> <li> <p>Accelerate and standardize the process of taking ML models to production</p> </li> <li> <p>Build scalable and high performance prediction services</p> </li> <li> <p>Continuously deploy, monitor, and operate prediction services in production</p> </li> </ol> <p>Learn more: BentoML</p> <p>Open Source Repository: GitHub</p> <p>Get Started on BentoML: Tutorial</p>"},{"location":"pipelines/bentoml/introduction-bentos.html#next-up","title":"Next up","text":"<p>Create Bentos</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"pipelines/elyra/elyra.html","title":"ML Pipeline Visual Editor Tool","text":""},{"location":"pipelines/elyra/elyra.html#overview","title":"Overview","text":"<p>ML Pipeline composability is an important feature while developing ML solutions. For a tool to have visualization support for ML pipeline composability, with drag and drop feature, makes it even more user friendly providing a no-code/low-code environment for development.</p> <p>Elyra is a set of AI-centric extensions to JupyterLab Notebooks. It is an open source ML pipeline visual editor tool developed by IBM, that converts multiple files into batch jobs or workflow.</p> Figure 1: Elyra Pipeline Composition <p>Elyra supports different types of files like Jupyter Notebook, Python, and R scripts for developing ML pipelines. It also supports the execution of these pipelines using JupyterLab, Kubeflow, or Apache Airflow.</p> <p>Note</p> <p>Elyra is built for visualization of end-end ML pipeline considering individual components i.e data collection,  cleaning, preprocessing, training, and evaluation to be run in a distributed manner. Hence it does not provide  shared memory support. Some cache support is available only for Kubeflow pipelines.  Cache is limited by the size of the shared files to be processed.</p> <p>Learn more: Elyra</p> <p>Open Source Repository: GitHub</p> <p>Get Started on Elyra: Tutorial</p>"},{"location":"pipelines/elyra/elyra.html#build-start-elyra","title":"Build &amp; Start Elyra","text":"<p>Note</p> <p>Password must be set for JupyterLab in the JUPYTER_TOKEN field of the .env file  before it is built and run. </p> <p>Use the following command to run Elyra tool</p> <pre><code>   make run-elyra\n</code></pre> <p>JupyterLab with Elyra extension can be accessed through the URL - http://0.0.0.0:8888/lab</p> Figure 2: Elyra Web Portal <p>BSD 3-Clause License: See License.</p>"},{"location":"pipelines/geti/deployment.html","title":"Deployment","text":""},{"location":"pipelines/geti/deployment.html#overview","title":"Overview","text":"<p>To demonstrate the capabilities of AiCSD, the Intel team has created a sample AI pipeline using Intel Geti models and SDK.</p> <p>With the Intel Geti platform, customers can train their own models for object detection, classification, segmentation, and anomaly. After the model has been trained, customers can export the models and deploy them seamlessly on AiCSD.</p> <p>Learn more: Intel Geti</p>"},{"location":"pipelines/geti/deployment.html#deploy-your-geti-model","title":"Deploy your Geti model","text":"<p>Once you export the trained model from Geti you can upload it to the AiCSD Web UI or add the model into the file system manually.</p>"},{"location":"pipelines/geti/deployment.html#web-ui-recommended","title":"Web UI (recommended)","text":"<p>Navigate to the Upload Models tab.</p> <p>Give the model a name, select <code>Geti</code> as the model type and upload the zip file exported from Geti using the Choose File button. Then click the Save button.</p> <p></p>"},{"location":"pipelines/geti/deployment.html#manual","title":"Manual","text":"<p>Create a subfolder under the models folder inside the AiCSD repo on the gateway machine. You can use any name, for example <code>models/cells</code>. </p> <p>After that, unzip the exported models file from Geti and copy the deployment folder inside the <code>models/cells</code> folder in the AiCSD repo.</p>"},{"location":"pipelines/geti/deployment.html#create-a-task-with-new-pipeline","title":"Create a task with new pipeline","text":"<p>To create a new task with the new deployed model, go to the task page on the AiCSD UI, and click on \"Add Task\".</p> <p>On the \"Pipeline\" dropdown menu, it will automatically appear the new model you deployed.</p> <p></p>"},{"location":"pipelines/geti/deployment.html#remaining-workflow","title":"Remaining Workflow","text":"<p>Now follow the rest of the Workflow as illustrated here</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"pipelines/ovms/ovms.html","title":"OVMS","text":""},{"location":"pipelines/ovms/ovms.html#overview","title":"Overview","text":"<p>Intel OpenVINO\u2122 Model Server (OVMS) is a high-performance system for serving machine learning models. It is based on C++ for high scalability and optimized for Intel solutions, so that you can take advantage of all the power of the Intel\u00ae Xeon\u00ae processor or Intel\u2019s AI accelerators and expose it over a network interface. OVMS uses the same architecture and API as TensorFlow Serving, while applying OpenVINO for inference execution. Inference service is provided via gRPC or REST API, making it easy to deploy new algorithms and AI experiments.</p> <p>OVMS is integrated into the AiCSD solution to enable inferencing using \u200bIntel\u00ae Distribution of OpenVINO\u2122 Toolkit</p> <p>Learn more: Intel OpenVINO\u2122 Model Server</p>"},{"location":"pipelines/ovms/ovms.html#deploy-ovms-model","title":"Deploy OVMS model","text":"<p>Firstly, convert the model into an OpenVINO\u2122 model. OVMS can only serve models that follow the OVMS directory structure. Please refer to the OVMS documentation for more details on this directory structure rule.</p> <p>To build the pipeline, use the Upload Models feature in the UI to upload zipped Geti/OVMS models. A new pipeline, with the same name as the model, should now be visible in the dropdown when creating a new Task as illustrated in Figure 1.</p> <p>After uploading the model, the unzipped folder will be available under the models folder. </p>"},{"location":"pipelines/ovms/ovms.html#create-a-task-with-new-pipeline","title":"Create a Task with New Pipeline","text":"<p>To create a new task with the new deployed model, follow instructions here.</p> <p>On the \"Pipeline\" dropdown menu, a new pipeline will automatically appear with the same name as the new model.</p> Figure 1: OVMS Pipelines"},{"location":"pipelines/ovms/ovms.html#remaining-workflow","title":"Remaining Workflow","text":"<p>Now follow the rest of the Workflow as illustrated here</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"security/security-docs.html","title":"Security Considerations","text":"<p>This guide will cover security considerations for the AiCSD Project.</p> <ol> <li>This Project is an Open Source Sample intended to be used as a potential use case/functionality, \"Art of the Possible\u201d. </li> <li>This is not intended to be a full solution; hence it deviates from the Intel SDLe Policy for Open Source Release of not meeting Production SDL. However, it is sufficient to be released as an Open Source Sample.</li> <li>This Open Source Sample is not intended to be used in a Production environment. Changes are required to improve security for a production environment.</li> </ol>"},{"location":"security/security-docs.html#security-implementation","title":"Security Implementation","text":"<p>This system is secured using the secure version of EdgeX Foundry. The EdgeX message bus is a secure Redis implementation.  In a single system setup, all services in the solution are configured as Add-on application services to Secure EdgeX. In a two system setup, the Gateway services are configured as add-on services, while the OEM services need to connect using SSH Tunneling. This implementation is an extension of the EdgeX example for Remote Device Services in Secure Mode.</p>"},{"location":"security/security-docs.html#additional-steps-to-secure-this-open-source-sample","title":"Additional steps to secure this Open Source Sample","text":"<p>The following list covers the additional features that need to be implemented to secure this open Source Sample:</p>"},{"location":"security/security-docs.html#oem","title":"OEM","text":"<ol> <li>Need to add an API gateway such as Kong to control access to OEM endpoints.</li> <li>Communications inside the OEM are not internally secure. Nothing prevents services from being called on localhost.</li> </ol>"},{"location":"security/security-docs.html#gateway","title":"Gateway","text":"<ol> <li>User Authentication needs to be implemented for the Web UI running on the Gateway.</li> <li>Communications inside the gateway are not internally secure. Kong provides an API Gateway for external callers, but does not prevent services from being called on localhost.</li> <li>The external MQTT broker for exporting results is not secure and should be replaced with a secure message broker.</li> <li>The Web UI operates over http not https meaning that network communications are not encrypted.</li> </ol>"},{"location":"security/security-docs.html#other","title":"Other","text":"<ol> <li>For data privacy guidelines, do not insert any personal identification content in job attributes like filename etc. </li> <li>Key pairs generated by ssh-keygen (4096-bit RSA keys) should be password protected.</li> <li>Key pairs should be unique to each set of systems and rotated at least every 3 years. </li> <li>There is currently no access control system in this project.</li> <li>Files should be encrypted and transferred over https instead of http.</li> <li>Implement TLS for Kong, Vault, and Redis (pub/sub) or a zero trust networking solution like OpenZiti in order to prevent the use of the secure port forwarding for communication between the OEM and Gateway.</li> </ol>"},{"location":"security/security-docs.html#aicsd-crash-mitigation","title":"AiCSD Crash Mitigation","text":"<ol> <li>The Monitoring, Log Analytics and Portainer features help monitor any failed/crashed docker containers.</li> <li>If either microservices fail, then end to end ML pipeline execution will fail.</li> <li>Hence the specific failed docker service can be restarted via Portainer. If the issue still persists, then all the docker containers can be restarted.</li> <li> <p>If the issue is still not resolved, run the following commands to remove all docker containers, images, volume, and image files. Archived input and output files are available in the archive folder on the Gateway. If required, files can be copied and then deleted from the OEM and Gateway file systems.</p> <pre><code> make down clean-volumes clean-images clean-files \n</code></pre> </li> <li> <p>Users can refer to Troubleshooting Services and Troubleshooting Tools sections to debug and resolve any issues encountered.</p> </li> </ol>"},{"location":"services/as-file-receiver-oem.html","title":"File Receiver OEM Application Service","text":""},{"location":"services/as-file-receiver-oem.html#overview","title":"Overview","text":"<p>The File Receiver OEM microservice receives the <code>Job(s)</code> object via the EdgeX Message Bus. It pulls the <code>Job</code> file(s) from the File Sender Gateway via the TransmitFile API endpoint. After a file is successfully written to the OEM system, it is archived on the Gateway.</p>"},{"location":"services/as-file-receiver-oem.html#dependencies","title":"Dependencies","text":"<p>This application service depends on the following services:</p> <ul> <li>Consul from EdgeX</li> <li>Redis for EdgeX Message Bus</li> <li>File Sender Gateway</li> <li>Job Repository</li> </ul>"},{"location":"services/as-file-receiver-oem.html#swagger-documentation","title":"Swagger Documentation","text":""},{"location":"services/as-file-receiver-oem.html#next-up","title":"Next up","text":"<p>Deep Dive into the Services - Web User Interface</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"services/as-file-sender-gateway.html","title":"File Sender Gateway Application Service","text":""},{"location":"services/as-file-sender-gateway.html#overview","title":"Overview","text":"<p>The File Sender Gateway microservice responds to the DataToHandle, TransmitFile, ArchiveFile, and RejectFile API endpoints. It sends the <code>Job</code> (received in DataToHandle) to the File Receiver OEM via the EdgeX Message Bus and accepts requests from the File Receiver OEM to pull the files once it receives the <code>Job</code>. After the output file(s) are successfully written to the OEM system, it is archived on the Gateway. If jobs are rejected in the Web-UI the File Sender Gateway copies the archived image to <code>$HOME/data/gateway-files/reject</code>.</p> <p>Note</p> <p>During the archival process, if the file is not a web viewable type (<code>.png, .jp(e)g, or .gif</code>), then an image conversion process is executed and a <code>.jpeg</code> image is created for use in the Web-UI.</p>"},{"location":"services/as-file-sender-gateway.html#dependencies","title":"Dependencies","text":"<p>This application service depends on the following services:</p> <ul> <li>Consul from EdgeX</li> <li>Redis for EdgeX Message Bus</li> <li>Job Repository</li> </ul>"},{"location":"services/as-file-sender-gateway.html#swagger-documentation","title":"Swagger Documentation","text":""},{"location":"services/as-file-sender-gateway.html#next-up","title":"Next up","text":"<p>Deep Dive into the Services - File Receiver OEM</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"services/as-pipeline-sim.html","title":"Pipeline Simulator Application Service","text":""},{"location":"services/as-pipeline-sim.html#overview","title":"Overview","text":"<p>The Pipeline Simulator is an alternative to a third party machine learning(ML) platform for development and integration purposes. It receives the <code>Launch</code> event and reading via the Message Bus, creates a simple copy of the input file as the output file(s). It then calls the Job Repo and Pipeline Status endpoints.</p> <p>The Pipeline Simulator also extends its functionality to support the Geti and BentoML sample ML pipelines. It receives an MQTT message to execute a Geti or BentoML sample pipelines. It also queries the OpenVINO Model Server(OVMS) to retrieve the available models and populate them as Geti or BentoML pipelines in the dropdown for creating a new task via the UI.</p>"},{"location":"services/as-pipeline-sim.html#dependencies","title":"Dependencies","text":"<p>This application service depends on the following services:</p> <ul> <li>Consul from EdgeX</li> <li>Redis for EdgeX Message Bus</li> <li>Job Repository</li> <li>Task Launcher</li> </ul>"},{"location":"services/as-pipeline-sim.html#swagger-documentation","title":"Swagger Documentation","text":""},{"location":"services/as-pipeline-sim.html#next-up","title":"Next up","text":"<p>Deep Dive into the Services - File Sender Gateway</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"services/as-pipeline-val.html","title":"Pipeline Validator","text":""},{"location":"services/as-pipeline-val.html#overview","title":"Overview","text":"<p>This Application Service is a simulator to use as an alternative to the microservice infrastructure for initial integration. The <code>StartPipeline</code> API will send an event to the pipeline to begin processing. This simulator will serve the endpoints necessary for the Job Repository Update and the Pipeline Status calls. The necessary integration points are outlined in the Creating Custom Pipelines section.</p>"},{"location":"services/as-pipeline-val.html#dependencies","title":"Dependencies","text":"<p>This application service depends on the following services:</p> <ul> <li>Consul from EdgeX</li> <li>Redis for EdgeX Message Bus</li> </ul>"},{"location":"services/as-pipeline-val.html#usage","title":"Usage","text":"<ol> <li>Build the Pipeline Validator and the Pipeline Simulator (if using).     <pre><code>make docker-pipeline-val docker-pipeline-sim\n</code></pre></li> <li> <p>[Optional] If using a custom pipeline service, modify the <code>APPLICATIONSETTINGS_PIPELINEHOST</code> and <code>APPLICATIONSETTINGS_PIPELINEPORT</code> variables in the file <code>docker-compose-pipeline-val.yml</code>.</p> <p>Note</p> <p>Refer to the table below to determine the values for <code>APPLICATIONSETTINGS_PIPELINEHOST</code> and <code>APPLICATIONSETTINGS_PIPELINEPORT</code> based on the desired Run Configuration. The default value is for the Pipeline Simulator.</p> </li> <li> <p>To use the pipeline validator, use one of the following run options:</p> Run Configuration Run Command APPLICATIONSETTINGS_PIPELINEHOST APPLICATIONSETTINGS_PIPELINEPORT Run with the pipeline simulator <code>make run-simulators</code> <code>pipeline-sim</code> <code>59789</code> Run with a custom pipeline service <code>make run-pipeline-val</code> <code>&lt;container_name&gt;</code> <code>&lt;Docker_network_port&gt;</code> </li> <li> <p>Open Postman and import the Postman collection from here.</p> </li> <li>Verify that the Pipeline API works, by sending the <code>Get Pipelines</code> request. This shows all pipelines and their topics.</li> <li> <p>Create or copy a file for the pipeline to process in <code>$HOME/data/gateway-files/input</code>.</p> <p>Note</p> <p>The location <code>$HOME/data/gateway-files</code> is volume mounted to <code>/tmp/files/</code> in the Docker container.</p> </li> <li> <p>Modify the <code>Launch Pipeline</code> request so that the body contains the correct file name and the appropriate mqtt topic for the selected pipeline.</p> </li> <li> <p>Click <code>Send</code> in Postman to send the request.</p> <p>Note</p> <p>To monitor the status, check the pipeline container log files.</p> </li> <li> <p>To check the status of the job, send the <code>Get Jobs</code> request. </p> </li> </ol>"},{"location":"services/as-pipeline-val.html#tear-down","title":"Tear Down","text":"<ol> <li>Stop the services running.     <pre><code>make down\n</code></pre></li> <li>Clean up the volumes. This is a destructive action, but will clear any configuration information that is in Consul.     <pre><code>make clean-volumes\n</code></pre></li> <li>Clean up the files. This is also destructive as it will clear the input directory and the output directory.     <pre><code>make clean-files\n</code></pre></li> </ol>"},{"location":"services/as-pipeline-val.html#swagger-documentation","title":"Swagger Documentation","text":""},{"location":"services/as-pipeline-val.html#future-considerations","title":"Future Considerations","text":"<p>In the future, the Pipeline Validator may have a UI to display the job information and start the pipeline(s). </p> <p>BSD 3-Clause License: See License.</p>"},{"location":"services/as-task-launcher.html","title":"Task Launcher Application Service","text":""},{"location":"services/as-task-launcher.html#overview","title":"Overview","text":"<p>The Task Launcher microservice manages and launches tasks for jobs to be executed on the Pipeline Simulator, Geti pipelines, or BentoML pipelines.</p>"},{"location":"services/as-task-launcher.html#dependencies","title":"Dependencies","text":"<p>This application service is dependent on the following services:</p> <ul> <li>Consul from EdgeX</li> <li>Redis from EdgeX</li> <li>Redis for EdgeX MessageBus</li> <li>Job Repostiory</li> <li>File Sender Gateway</li> </ul> <p>Note</p> <p>The same Redis implementation is used both for the database and the Publish/Subscribe Message Broker needed for the EdgeX Message Bus.</p>"},{"location":"services/as-task-launcher.html#configuration","title":"Configuration","text":"<p>Change task launcher configurations in the configuration.toml file. Configuration options can also be changed when the service is running by using Consul.</p> <p>Note</p> <p>For changes to take effect, the service must be restarted. If changes are made to the configuration.toml file, the service must be stopped, rebuilt, and started again.</p> <ul> <li>RetryWindow: Determines how often a job should be resent to the pipeline for processing</li> <li>DeviceProfileName: Indicates the device profile information for the pipeline to consume</li> <li>DeviceName: Indicates the device name for the pipeline to consume</li> </ul>"},{"location":"services/as-task-launcher.html#swagger-documentation","title":"Swagger Documentation","text":""},{"location":"services/as-task-launcher.html#next-up","title":"Next up","text":"<p>Deep Dive into the Services - Pipeline Simulator</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"services/general-info.html","title":"General Information","text":"<p>This guide provides general information for the overall collection of services at a system-level. Click on the  hyperlinked name of the microservice in the Port Configurations table below for service-level information.</p>"},{"location":"services/general-info.html#image-processing","title":"Image Processing","text":"<p>Images are processed and tracked using jobs. The job tracks the movement of the file, the status, and any results or outputs  from the pipeline. To process a job, there are tasks that help match information about a job to the appropriate pipeline  to run. The animation below demonstrates how a file gets automatically processed.</p> <p></p> <ol> <li> <p>The Input Device/Imager writes the file to the OEM file system in a directory that is watched by the File Watcher. When  the File Watcher detects the file, it sends the job (JSON struct of particular fields) to the Data Organizer via HTTP Request.</p> </li> <li> <p>The Data Organizer sends the job to the Job Repository to create the job in the Redis Database. The job information is then  sent to the Task Launcher to determine if there is a task that matches the job. If there is, the job may proceed to the  File Sender (OEM). </p> </li> <li> <p>The File Sender (OEM) is responsible for sending both the job and the file to the File Receiver (Gateway). Once the File Receiver (Gateway) has written the file to the Gateway file system, the job may then be sent on to the Task Launcher. </p> </li> <li> <p>The Task Launcher verifies that there is a matching task for the job before sending it to the appropriate pipeline using the EdgeX Message Bus (via Redis). The ML pipeline subscribes to the appropriate topic and processes the file in its pipeline. The output file (if there is one) is written to the file system and the job is sent back to the Task Launcher. </p> </li> <li> <p>The Task Launcher then decides if there is an output file or if there are just results. In the case of only results and no output file, the Task Launcher marks the job as complete. If there is an output file, the Task Launcher sends the job onwards to the File Sender (Gateway). </p> </li> <li> <p>The File Sender (Gateway) publishes the job information to the EdgeX Message Bus via Redis for the File Receiver (OEM) to subscribe and pull. The File Receiver (OEM) sends an HTTP request to the File Sender (Gateway) for the output file(s).  The file(s) are sent as part of the response and the File Receiver (OEM) writes the output file(s) to the file system.</p> </li> </ol>"},{"location":"services/general-info.html#port-configurations","title":"Port Configurations","text":"Microservice Port file-watcher 59780 data-organizer 59781 file-sender-oem 59782 file-receiver-gateway 59783 job-repository 59784 task-launcher 59785 file-sender-gateway 59786 file-receiver-oem 59787 web-ui 4200 pipeline-sim 10107 pipeline-val 59788 bentoml-services 3000 Dependencies Port Consul 8500 Redis 6379 App MQTT Export 59703 External MQTT Broker 1883 InfluxDB 8086 Grafana 3001 Kibana 5601 OpenVINO Model Server(OVMS) 9001 (grpc), 9002 (http) Elyra 8888 EdegX Security Services Port Which System Kong 8000, 8443 Gateway or Single System Kong DB 5432 Gateway or Single System Security Proxy Setup None Gateway or Single System Security SecretStore Setup None Gateway or Single System Security Bootstrapper None Gateway or Single System Vault 8200 Gateway or Single System Security Proxy Setup 59780-59782, 59787 Gateway Only Security Spiffe Token Provider 59841 Gateway Only Security Spire Agent None Gateway Only Security Spire Config None Gateway Only Security Spire Server 59840 Gateway Only SSHd Remote 2223 OEM Only Remote Spire Agent None OEM Only <p>Note</p> <p>For more information on the Security Services for EdgeX, refer to the EdgeX Security Documentation. The two system secure port forwarding is implemented based on the EdgeX example for remote device services in secure mode.</p>"},{"location":"services/general-info.html#build-options","title":"Build Options","text":"<p>The top level Makefile contains the following make targets for building the service binaries and docker images.</p> Option Description <code>make tidy</code> Runs <code>go mod tidy</code> to ensure the go.sum file is up to date. Only needed once if <code>build</code> fails due to a go.sum issue. <code>make build</code> Builds all the AiCSD microservice binaries. <code>make &lt;service-name&gt;</code> Builds the specified microservice binary. Microservice names listed in the table above are used as the make targets. The <code>&lt;service-name&gt;</code> is any name listed in the Microservices table. <code>make docker</code> Builds all AiCSD microservice docker images. Adding the option <code>-j&lt;threads&gt;</code> will tell make how many commands to run in parallel, where <code>&lt;threads&gt;</code> is the desired number of threads. <code>make docker-build-gateway</code> Builds the AiCSD Gateway specific microservice docker images. Adding the option <code>-j&lt;threads&gt;</code> will tell make how many commands to run in parallel, where <code>&lt;threads&gt;</code> is the desired number of threads. <code>make docker-build-oem</code> Builds the AiCSD OEM specific microservice docker images. Adding the option <code>-j&lt;threads&gt;</code> will tell make how many commands to run in parallel, where <code>&lt;threads&gt;</code> is the desired number of threads. <code>make docker-&lt;service-name&gt;</code> Builds the specified microservice docker image. The <code>&lt;service-name&gt;</code> is any name listed in the Microservices table. <code>make files</code> Creates local folders for the OEM and Gateway files. Dependency of the <code>run*</code> targets below."},{"location":"services/general-info.html#run-options","title":"Run Options","text":"<p>The top level Makefile contains the following make targets for running the microservices in docker.</p> <p>Note</p> <p>The AiCSD docker images are not pushed to any Docker Registry. They must be built locally prior to using the target(s) below that depend on those docker images.</p> Option Description <code>make run-gateway GATEWAY_IP_ADDR=192.168.XX.XX</code> Runs all the Gateway targeted service containers including EdgeX and AiCSD services. Intended for use with a separate OEM system and a custom pipeline configuration. <code>make run-gateway-sim</code> Runs the Gateway services with a pipeline simulator. Used for integration testing or development with a separate OEM system. <code>make run-gateway-geti</code> Runs the Gateway services with Intel Geti for pipeline creation. Requires a separate OEM system. <code>make run-ovms</code> Runs the AiCSD Gateway services with a pipeline simulator. Used for integration testing or development with a separate OEM system. <code>make run GATEWAY_IP_ADDR=192.168.XX.XX</code> Runs the OEM and Gateway targeted service containers including EdgeX and AiCSD services on a single system. Intended for use with a separate OEM system and a custom pipeline configuration. <code>make run-sim</code> Runs the OEM and Gateway services with a pipeline simulator. Used for integration testing or development. <code>make run-geti</code> Runs the OEM and Gateway services with Intel Geti for pipeline creation. <code>make run-oem</code> Runs the OEM services. Used for integration testing or development with a separate Gateway system. <p>Note</p> <p>Appending the gateway IP Address <code>GATEWAY_IP_ADDR=192.168.XX.XX</code> will allow the web UI to be accessed from a remote system. To get the Gateway IP Address run <code>hostname -I</code> in a terminal on the Gateway system.</p>"},{"location":"services/general-info.html#clean-up-options","title":"Clean-Up Options","text":"<p>The following options are available for tearing down and cleaning up the solution.</p> Option Description <code>make down</code> Stops all containers no matter which target was used to start them. <code>make down-oem</code> Stops all OEM containers. <code>make down-clean</code> Stops all containers and removes all the volumes. This will result in data loss. <code>make clean-files</code> Removes the local folders for the OEM and Gateway files. Removes EdgeX Secrets with sudo permissions. This will result in data loss and any configured pipelines will be lost. <code>make clean-volumes</code> Removes all unused Docker volumes. This will result in data loss. This command will work successfully for Docker version &gt;= 23.0. However for Docker version &lt; 23.0 use the following command: <code>docker volume --prune</code> <code>make clean-images</code> Removes all the locally built AiCSD Docker images. <code>make clean-builders</code> Removes all the \"builder\" images left over from the docker build process. <code>make clean-keys</code> Removes all the ssh keys from their directories for either the Gateway or OEM system."},{"location":"services/general-info.html#portainer","title":"Portainer","text":"<p>Portainer is a service with a web UI that can be used for container management.</p> Option Description <code>make run-portainer</code> Runs the Portainer container management application independent of the AiCSD services. <code>make down-portainer</code> Stops the Portainer container."},{"location":"services/general-info.html#validation","title":"Validation","text":"<p>The following validation test options are used to run unit and integration tests. For test reports, <code>go-test-report</code> is used to write test output to html files.</p> Option Description <code>make test</code> Runs the unit tests locally. <code>make integration-test</code> Runs the basic integration tests. <code>make integration-test-retry</code> Runs all integration tests including the retry test cases that will test the starting and stopping of the services. Improper synchronization of the containers stopping and starting can cause these tests to fail. <code>make unit-test-report</code> Runs the unit tests locally and outputs the results and coverage to unit-test-report.html and unit-test-cov-report.html respectively. <code>make integration-test-report</code> Runs all integration tests including the retry test cases that will test the starting and stopping of the services. The results are output to integration-test-report.html. <code>make test-report</code> Runs the unit tests and integration test targets. It will generate output files unit-test-report.html for the unit tests, unit-test-cov-report.html for the unit test coverage and integration-test-report.html for the integration tests. <code>make copy-files COPY_DIR=/path/to/input-images/to-copy SLEEP_TIME=&lt;time in seconds&gt;</code> Used for manual testing on the OEM/single system setup for copying files from a specified COPY_DIR to the appropriate location while waiting the specified SLEEP_TIME (default 30s)."},{"location":"services/general-info.html#run-integration-tests-in-a-docker-container","title":"Run Integration Tests in a Docker Container","text":"<ol> <li> <p>Build the docker image:</p> <pre><code>make docker-integration-test\n</code></pre> </li> <li> <p>Run the container:</p> <pre><code>docker run --net host -v /var/run/docker.sock:/var/run/docker.sock -it --entrypoint=\"/bin/sh\" aicsd/integration-test:0.0.0-dev\n</code></pre> </li> <li> <p>Once inside the shell, execute:</p> <pre><code>make integration-test\n</code></pre> </li> </ol>"},{"location":"services/general-info.html#services-fault-tolerance","title":"Services Fault Tolerance","text":"<p>The services use a Go module called <code>wait-for-it</code> to wait on the availability of a TCP host and port. The <code>wait-for-it</code> Go module is added to the microservices so that the services may wait for their dependencies to be up and ready as expected. It currently has a 15-second timeout and provides feedback as the dependent services become ready. If for some reason a service never becomes available, there is a one-minute maximum timeout, after which an error will be logged that a dependent service never became available.</p>"},{"location":"services/general-info.html#documentation-using-github-pages","title":"Documentation Using GitHub Pages","text":"<p>This repo leverages a GitHub Pages approach to represent markdown contents as navigable html web pages. To build and view the documentation locally, use:</p> <pre><code>make serve-docs\n</code></pre> <p>Tip</p> <p>Open a browser to view the contents: localhost:8008</p>"},{"location":"services/general-info.html#next-up","title":"Next up","text":"<p>Want to start a deep dive into services? See File Watcher.</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"services/integration-tests.html","title":"Integration Test","text":"<p>Integration tests are automated to ensure that the containers work together properly. All these tests use a simulated pipeline to demonstrate that data can be transferred throughout the system. Tests may be found in the <code>integration-tests</code> directory. This directory also contains a Postman collection that may be imported to run additional manual integration tests.</p>"},{"location":"services/integration-tests.html#run-integration-tests","title":"Run Integration Tests","text":""},{"location":"services/integration-tests.html#prerequisites","title":"Prerequisites","text":"<p>Ensure that there is a clean environment.</p> <ol> <li>Stop all services: </li> </ol> <pre><code>make down\n</code></pre> <ol> <li> <p>To clean up the files from the directories on the Linux system, run:</p> <pre><code>make clean-files\n</code></pre> <p>Warning</p> <p>This is a destructive action and will delete any existing files.</p> </li> <li> <p>To clean up the docker images, run:     <pre><code>make clean-images\n</code></pre></p> </li> <li> <p>To remove any volumes from the system, run:</p> <pre><code>make clean-volumes\n</code></pre> <p>Warning</p> <p>This is a destructive action and will clean up any unused docker volumes. This will delete the database and all its contents when run.</p> </li> <li> <p>Use one of the command line options below to build: </p> General Build <pre><code>   make docker\n</code></pre> Fast Build <pre><code>  make -j&lt;num of threads&gt; docker\n</code></pre> Build Option When To Use General Build The system resources are unknown. Fast Build The flag -j represents the number of jobs in a build. The optimal integer for the number of jobs varies, depending on the system resources (e.g., cores) and configuration. </li> </ol>"},{"location":"services/integration-tests.html#run-makefile-targets","title":"Run Makefile Targets","text":"<p>There are several commands that may be used to run different configurations of the integration tests:</p> Integration Test using Simulators Run Option Description <code>make integration-test</code> Runs integration tests using the AiCSD simulators. <code>make integration-test-pipeline-sim</code> Runs the integration tests using the Pipeline Simulator. <code>make integration-test-retry</code> Runs the retry integration tests. <p>Note</p> <p>The retry integration test target runs additional tests that include starting and stopping the containers to orchestrate more complex integration tests.</p>"},{"location":"services/integration-tests.html#run-natively","title":"Run Natively","text":"<ol> <li> <p>Change to the integration test directory:</p> <pre><code>  cd integration-tests\n</code></pre> </li> <li> <p>Run the integration tests: </p> <pre><code>  go test ./...\n</code></pre> </li> </ol>"},{"location":"services/integration-tests.html#run-within-a-docker-container","title":"Run within a Docker Container","text":"<ol> <li> <p>Make the necessary directories:</p> <p><pre><code>   make files\n</code></pre> 2. Start the integration test container: </p> <pre><code>docker run -it -v $(pwd):/app  -v /var/run/docker.sock:/var/run/docker.sock -v ${HOME}:${HOME} --net=host aicsd/integration-tests:0.0.0-dev /bin/ash\n</code></pre> </li> <li> <p>Navigate to the <code>app</code> directory:</p> <pre><code>   cd app\n</code></pre> </li> <li> <p>Run the desired integration test target:</p> </li> </ol> Run Option Description <code>make integration-test</code> Runs integration tests using the AiCSD simulators. <code>make integration-test-pipeline-sim</code> Runs the integration tests using the Pipeline Simulator. <code>make integration-test-retry</code> Runs the retry integration tests."},{"location":"services/integration-tests.html#postman","title":"Postman","text":"<p>The file <code>AiCSD.postman_collection.json</code> contains requests for manually testing using PostMan. The contents of this file will grow over time.</p>"},{"location":"services/integration-tests.html#usage","title":"Usage","text":"<ol> <li>From your Workspace in the PostMan application select Import and navigate to select the above collection file. </li> <li>Use the requests in the imported collection to create new Tasks, Get all Tasks, Get all Jobs, etc.</li> </ol>"},{"location":"services/integration-tests.html#chaos-testing","title":"Chaos Testing","text":"<p>Pumba is a command-line, chaos-testing tool for Docker. It can be used to stress test containers, emulate network failures and modify the containers running. Collecting metrics from different workloads within these test types assesses the robustness of the containers.</p>"},{"location":"services/integration-tests.html#getting-started-with-pumba","title":"Getting Started with Pumba","text":"<ol> <li> <p>Clone the repository:     <pre><code>git clone https://github.com/MUCZ/pumba\n</code></pre></p> <p>Note</p> <p>This is a specific fork of the repository <code>alexei-led/pumba.git</code> with compiler changes for <code>go-1.18</code>.</p> </li> <li> <p>Change directories:     <pre><code>cd pumba\n</code></pre></p> </li> <li>Build the Pumba binary:     <pre><code>make\n</code></pre></li> <li>The binary may be found at <code>.bin/github.com/alexei-led</code>. To test the binary, print out the help resources:     <pre><code>./pumba --help\n</code></pre></li> </ol>"},{"location":"services/integration-tests.html#test-with-pumba","title":"Test with Pumba","text":"<p>Use Pumba to control containers:</p> <ul> <li><code>stop</code>: stops specified containers</li> <li><code>kill</code>: kills specified containers</li> <li><code>rm</code>: removes specified containers</li> <li><code>netem</code>: emulates network variances using <code>tc</code> traffic control tool</li> </ul> <p>Use Pumba to control system resources:</p> <ul> <li><code>stress</code>: stresses the specified containers using <code>stress-ng</code></li> <li><code>pause</code>: pauses all processes</li> </ul>"},{"location":"services/integration-tests.html#sample-commands","title":"Sample Commands","text":"<p>Network Emulation: The following workload uses a docker image to host the traffic control commands. The workload simulates a network delay of <code>30 ms</code> with a <code>5 ms</code> delay variation (jitter) that is normally distributed. To find the containers to target, the following regex expression will select all containers that start with <code>edgex</code>.     <pre><code>./pumba netem --duration 1m --tc-image gaiadocker/iproute2 --interface eth1 delay --time 30 --jitter 5 --distribution normal \"re2:^edgex\"\n</code></pre></p> <p>Stress Test: The following will run a one minute stress test on the containers whose names start with <code>edgex</code>.</p> <pre><code>./pumba stress --stress-image alexeiled/stress-ng:latest-ubuntu -d 1m \"re2:^edgex\"\n</code></pre>"},{"location":"services/integration-tests.html#pumba-resources","title":"Pumba Resources","text":"<p>Pumba Repository</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"services/ms-data-organizer.html","title":"Data Organizer Microservice","text":""},{"location":"services/ms-data-organizer.html#overview","title":"Overview","text":"<p>The Data Organizer microservice routes calls from other microservices to the job repository. It acts as the gatekeeper and controller, determining if a call should create a new job in the job repository. Upon getting a call from the File Watcher, it queries the Task Launcher to determine if there are any tasks that match the input file. If no tasks match, the call errors out. If there is a matching task, the Data Organizer sends the job onwards to the File Sender OEM.</p>"},{"location":"services/ms-data-organizer.html#dependencies","title":"Dependencies","text":"<p>This application service depends on the following services:</p> <ul> <li>Consul from EdgeX</li> <li>Job Repository</li> <li>Task Launcher</li> <li>File Sender OEM</li> </ul>"},{"location":"services/ms-data-organizer.html#attribute-parser","title":"Attribute Parser","text":"<p>This service parses the filename of a job for attributes. To enable this process, specify a parsing schema in the <code>ms-data-organizer</code> configuration TOML under the <code>[AttributesParser]</code> section.</p> <p>The Attribute requires three elements:</p> <ul> <li>Name</li> <li>ID</li> <li>DataType</li> </ul> <pre><code>[AttributeParser]\n   [AttributeParser.Name]\n   Id=\"Set search key letter/phrase\"\n   DataType=\"data type of the value (bool, int, or string)\"\n</code></pre> <p>Example</p> <p>In this example, the Names for each attribute are Flag, Number, and Operator. </p> <p>The attributes have an associated ID and DateType.</p> <pre><code>[AttributeParser]\n  [AttributeParser.Flag]\n  Id=\"f\"\n  DataType=\"bool\"\n  [AttributeParser.Number]\n  Id=\"n\"\n  DataType=\"int\"\n  [AttributeParser.Operator]\n  Id=\"op\"\n  DataType=\"string\"\n</code></pre> <p>When the TOML is filled out correctly, the system automatically parses incoming filenames for their attributes using the given schema. It searches for the <code>Id</code> in the file name and collects the corresponding data based on the <code>DataType</code>. It stores the data in a <code>map[string]string{AttributeName: data}</code>:</p> <p>Example</p> <p>Input File: op-Bob-n007-f.tiff</p> <p>Resulting Attributes:   <pre><code>\"Flag\": \"true\",\n\"Number\": \"7\",\n\"Operator\": \"Bob\"\n</code></pre></p>"},{"location":"services/ms-data-organizer.html#swagger-documentation","title":"Swagger Documentation","text":""},{"location":"services/ms-data-organizer.html#next-up","title":"Next up","text":"<p>Deep Dive into the Services - Job Repository</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"services/ms-file-receiver-gateway.html","title":"File Receiver Gateway Microservice","text":""},{"location":"services/ms-file-receiver-gateway.html#overview","title":"Overview","text":"<p>The File Receiver Gateway microservice responds to TransmitJob and TransmitFile API endpoints. On startup, it queries for unprocessed job events. The File Receiver Gateway writes files sent to it from the File Sender OEM to the output directory specified in the configuration.toml file.</p>"},{"location":"services/ms-file-receiver-gateway.html#dependencies","title":"Dependencies","text":"<p>This application service depends on the following services:</p> <ul> <li>Consul from EdgeX</li> <li>Job Repository</li> <li>Task Launcher</li> </ul>"},{"location":"services/ms-file-receiver-gateway.html#swagger-documentation","title":"Swagger Documentation","text":""},{"location":"services/ms-file-receiver-gateway.html#next-up","title":"Next up","text":"<p>Deep Dive into the Services - Task Launcher</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"services/ms-file-sender-oem.html","title":"File Sender OEM Microservice","text":""},{"location":"services/ms-file-sender-oem.html#overview","title":"Overview","text":"<p>The File Sender OEM microservice listens for events and sends files from those events. On startup, it queries for unprocessed job events. The File Sender OEM sends files received from the data organizer to the  File Receiver Gateway. The configuration information is set in the configuration.toml file.</p>"},{"location":"services/ms-file-sender-oem.html#dependencies","title":"Dependencies","text":"<p>This application service depends on the following services:</p> <ul> <li>Consul from EdgeX</li> <li>Job Repository</li> <li>File Receiver Gateway</li> </ul>"},{"location":"services/ms-file-sender-oem.html#swagger-documentation","title":"Swagger Documentation","text":""},{"location":"services/ms-file-sender-oem.html#next-up","title":"Next up","text":"<p>Deep Dive into the Services - File Receiver Gateway</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"services/ms-file-watcher.html","title":"File Watcher Microservice","text":""},{"location":"services/ms-file-watcher.html#overview","title":"Overview","text":"<p>The File Watcher microservice watches specified folders for new files. Upon startup, it queries selected folder(s) on a local system for unprocessed files. When it identifies a new file(s), it makes a REST call to the data organizer with the job containing the new file information. </p>"},{"location":"services/ms-file-watcher.html#dependencies","title":"Dependencies","text":"<p>This application service depends on the following services:</p> <ul> <li>Consul from EdgeX</li> <li>Data Organizer</li> </ul>"},{"location":"services/ms-file-watcher.html#configuration","title":"Configuration","text":"<p>The File Watcher microservice has a number of configurations that can be changed in the configuration.toml file. For most of these changes to be reflected, the File Watcher container must be restarted. However, the settings listed below can be manipulated while the service is running by using Consul <code>Key/Values/edgex/appservices/2.0/ms-file-watcher/</code>:</p> <ul> <li>WatchSubfolders: Alerts the microservice to search through nested file structures.</li> <li>FileExclusionList: Blocks certain files from being processed with a comma-separated list of substrings. </li> <li>LogLevel: Determines verbosity of logging output.</li> </ul> <p>Example</p> <p>** FileExclusionList Substrings **</p> <p><code>FileExclusionList=\"test, image-4k\"</code></p> <p>These files would not be processed as their names contain whole substrings from the file exclusion list:</p> <p><code>test-image.tiff</code> and <code>image-4k.tiff</code> </p> <p>This file would be processed:</p> <p><code>image.tiff</code></p>"},{"location":"services/ms-file-watcher.html#usage","title":"Usage","text":"<p>This Device Service runs standalone or with EdgeX services. It must have communication via REST API to the data organizer in order for data to be processed.</p>"},{"location":"services/ms-file-watcher.html#next-up","title":"Next up","text":"<p>Deep Dive into the Services - Data Organizer</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"services/ms-job-repository.html","title":"Job Repository Microservice","text":""},{"location":"services/ms-job-repository.html#overview","title":"Overview","text":"<p>The Job Repository microservice provides a central location for managing information related to the processing of files/jobs.</p>"},{"location":"services/ms-job-repository.html#dependencies","title":"Dependencies","text":"<p>This application service depends on the following services:</p> <ul> <li>Consul from EdgeX</li> <li>Redis from EdgeX</li> </ul>"},{"location":"services/ms-job-repository.html#swagger-documentation","title":"Swagger Documentation","text":""},{"location":"services/ms-job-repository.html#next-up","title":"Next up","text":"<p>Deep Dive into the Services - File Sender OEM</p> <p>BSD 3-Clause License: See License.</p>"},{"location":"services/ms-web-ui.html","title":"Web UI","text":"<p>The AiCSD Web UI is an Angular interface that provides features such as:</p> <ul> <li>Model - Upload</li> <li>Task - Create, Update, Delete</li> <li>Job - Monitor, Filter, Verify</li> <li>Dashboard - View Analytics</li> </ul>"},{"location":"services/ms-web-ui.html#how-do-i-build-this-thing","title":"How do I build this thing?","text":"<p>This project is intended to be built and run using only <code>make</code> and <code>docker</code> without the need for installing <code>npm</code>, <code>nodejs</code>, or <code>angular-cli</code>.</p> <p>To skip the technical details, jump to Initial Setup</p> <p>If you want to know how the heck this thing works and what it is doing to your machine,  read on to Build Ideology</p>"},{"location":"services/ms-web-ui.html#build-ideology","title":"Build Ideology","text":"<p>A lot of work has gone into making this friendlier to build and run. <code>Docker</code> and <code>make</code> are used to allow the developer to work without ever installing <code>nodejs</code> or <code>angular-cli</code>.</p> <p>A base nodejs docker image is built using your local user and group. When this image is run, this entire repo is mounted under <code>/app</code> inside the container. This means whatever you do inside it is also done locally as well. However, it also means that whatever you do locally is automatically available to the docker side of things. This allows the developers to work completely dockerized, completely native, or somewhere in-between.</p> <p>Two docker files are provided, a dev one and a production one. By default, the dev one will be used. View the Production section to see how to build and run the production version.</p> <p><code>make</code> commands are used to abstract away the more complicated docker command line. </p> <p><code>make debug</code> will launch an interactive build container to run any commands you would like (<code>npm</code>, <code>ng</code>, etc).</p> <p><code>make ng=\"...\"</code> and <code>make npm=\"...\"</code> allow you to run ng and npm commands within the confines of the docker build container.</p> <p>Note</p> <p>Following make commands with target as <code>serve</code>, <code>test</code>, and <code>debug</code> will work only on the gateway system where the Web-UI microservice is built and run.</p>"},{"location":"services/ms-web-ui.html#initial-setup","title":"Initial Setup","text":""},{"location":"services/ms-web-ui.html#create-development-docker-image","title":"Create development docker image","text":"<pre><code>make image\n</code></pre>"},{"location":"services/ms-web-ui.html#install-npm-packages-node_modules","title":"Install npm packages (node_modules)","text":"<pre><code>make install\n</code></pre>"},{"location":"services/ms-web-ui.html#build-and-serve-dev-code","title":"Build and serve dev code","text":"<pre><code># Foreground\nmake serve\n</code></pre>"},{"location":"services/ms-web-ui.html#view-website","title":"View Website","text":"<p>The website is available at http://localhost:4200.</p> <p>Any changes to the code will be hot-reloaded by the angular dev server (except in production mode).</p>"},{"location":"services/ms-web-ui.html#internationalization","title":"Internationalization","text":"<p>The website supports the localization of many Job fields to Chinese. To enable Chinese localization,  set the <code>Accept-Language</code> header value to <code>zh</code>. This may be set by adding an extension such as Mod Header to the current web browser, or via <code>Postman</code>, or <code>curl</code>. Once the header configuration is set, view the localized Job fields under the Jobs tab of the UI.</p>"},{"location":"services/ms-web-ui.html#stop-remove-containers","title":"Stop / Remove Containers","text":"<p>In order to stop the angular server which is in the foreground, use <code>Ctrl-C</code>. If that does not work, you can stop the running container with:</p> <pre><code>docker stop &lt;container_name&gt;\n</code></pre>"},{"location":"services/ms-web-ui.html#other-useful-make-commands","title":"Other useful make commands","text":"<pre><code># Build and run unit tests\nmake test\n\n# Open the unit test code coverage in your browser\nmake view-coverage\n\n# Run linter\nmake lint\n\n# Check for security issues with node packages\nmake audit\n\n# Attempt to fix security issues with packages\nmake audit-fix\n\n# Upgrade angular to the latest stable version\nmake update-angular\n\n# Clean build cache, coverage info and dist files\nmake clean\n\n# Clear out all node modules and build artifacts\nmake nuke\n\n# Run an npm install\nmake install\n\n# Run an npm install --force\nmake force-install\n\n# Open an interactive build environment\n# Desktop x11 environment is mapped and programs such as google-chrome can be ran in GUI mode\nmake debug\n\n# Generate documentation\nmake docs\n\n# Open documentation in web-browser\nmake view-docs\n</code></pre>"},{"location":"services/ms-web-ui.html#advanced-make-commands","title":"Advanced Make Commands","text":"<p>Open an interactive build environment to run any <code>npm</code>, <code>ng</code>, or other command within the confines of the volume mounted dev container</p> <pre><code>make debug\n</code></pre> <p>To run <code>npm</code> commands within the confines of the volume mounted dev container, set the <code>npm</code> variable to your <code>npm</code> argument list like so:</p> <pre><code># This will run \"npm audit --fix\"\nmake npm=\"audit --fix\"\n\n# This will run \"npm install [package-name] --save\"\nmake npm=\"install moment --save\"\n</code></pre> <p>To run <code>angular-cli</code> (<code>ng</code>) commands within the confines of the volume mounted dev container, set the <code>ng</code> variable to your <code>ng</code> argument list like so:</p> <pre><code># This will run \"ng generate component Test\"\nmake ng=\"generate component Test\"\n</code></pre> <p>To run any arbitrary command within the confines of the volume mounted dev container, set the <code>exec</code> variable to your command and argument list like so:</p> <pre><code># This will run \"node --version\"\nmake exec=\"node --version\"\n</code></pre>"},{"location":"services/ms-web-ui.html#code-generation","title":"Code Generation","text":"<p>Run <code>make gen=\"component component-name\"</code> to generate a new component.</p> <p>You can generate the following schematics:</p> <ul> <li>app-shell</li> <li>application</li> <li>class</li> <li>component</li> <li>directive</li> <li>enum</li> <li>guard</li> <li>interceptor</li> <li>interface</li> <li>library</li> <li>module</li> <li>pipe</li> <li>resolver</li> <li>service</li> <li>service-worker</li> <li>web-worker</li> </ul> <p>Note: Do not include the suffix (<code>Component</code>, <code>Service</code>, etc.). They are added for you</p>"},{"location":"services/ms-web-ui.html#testing","title":"Testing","text":"<p>Run <code>make test</code> to execute the unit tests via Karma. You can view the Karma tests at the address http://127.0.0.1:9876. Changes will be hot-reloaded and tests will be re-run.</p> <p>Run <code>make view-coverage</code> to view static code coverage HTML output. Alternatively, open /ms-web-ui/coverage/report-html/index.html under project location, with your browser.</p> <p>Note: In order for the code coverage to be accurate, you must first refresh the Karma unit test service at http://127.0.0.1:9876. It will then produce the HTML code coverage output for you to view in your browser as an HTML file.</p>"},{"location":"services/ms-web-ui.html#native-tooling","title":"Native Tooling","text":"<p>In order to set up a native tooling environment without docker (for instance, use with an IDE), follow these steps to download and install all dependencies.</p>"},{"location":"services/ms-web-ui.html#install-nodejs-lts","title":"Install NodeJS LTS","text":"<ul> <li>[Recommended] Install from binary distributions</li> <li>Install via snap</li> <li>Note: Installations via snap will sometimes cause issues with installing global packages such as <code>angular-cli</code></li> </ul>"},{"location":"services/ms-web-ui.html#install-angular-cli","title":"Install Angular CLI","text":"<pre><code>npm install -g @angular/cli\n</code></pre>"},{"location":"services/ms-web-ui.html#production","title":"Production","text":"<p>The production mode generates production built angular static html/js files able to be served via <code>nginx</code> or similar web server.</p>"},{"location":"services/ms-web-ui.html#build-production-files","title":"Build production files","text":"<pre><code>make dist\n</code></pre>"},{"location":"services/ms-web-ui.html#next-up","title":"Next up","text":"<p>Deep Dive into the Services - Integration Tests</p> <p>BSD 3-Clause License: See License.</p>"}]}